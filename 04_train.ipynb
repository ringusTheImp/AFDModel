{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# WX-AFD: Fine-Tuning Qwen3-4B for Area Forecast Discussions\n\nThis notebook fine-tunes **Qwen3-4B-Instruct-2507** with DoRA + rsLoRA to generate NWS Area Forecast Discussions from structured weather data.\n\n**Pipeline context:** This is step 4 in the WX-AFD pipeline:\n1. `01_scrape_afds.py` — Scrape AFDs from IEM\n2. `02_fetch_weather.py` — Fetch weather data from Open-Meteo\n3. `03_build_dataset.py` — Build training JSONL (messages format)\n4. **`04_train.ipynb` — Fine-tune, evaluate, and export model** (this notebook)\n\n---\n\n**Table of Contents**\n1. [Data Inspection](#1.-Data-Inspection)\n2. [Configuration](#2.-Configuration)\n3. [Sanity Check](#3.-Sanity-Check)\n4. [Training](#4.-Training)\n5. [Training Curves](#5.-Training-Curves)\n6. [LoRA Merge](#6.-LoRA-Merge)\n7. [Inference](#7.-Inference)\n8. [Evaluation](#8.-Evaluation)\n9. [Results](#9.-Results)\n10. [Next Steps](#10.-Next-Steps)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import json\nimport os\nimport subprocess\nimport sys\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n# ---- Paths ----\n# Adjust WX_AFD_ROOT if running outside Derecho\nWX_AFD_ROOT = Path(os.environ.get(\n    \"WX_AFD_ROOT\",\n    f\"/glade/derecho/scratch/{os.environ['USER']}/wx-afd\"\n))\nDATA_DIR = WX_AFD_ROOT / \"data\"\nCONFIG_PATH = WX_AFD_ROOT / \"configs\" / \"wx-afd-dora.yml\"\nOUTPUT_DIR = WX_AFD_ROOT / \"output\"\nEVAL_DIR = WX_AFD_ROOT / \"eval\"\n\nMODEL_ID = \"Qwen/Qwen3-4B-Instruct-2507\"\n\n# ---- GPU check ----\nprint(f\"PyTorch:  {torch.__version__}\")\nprint(f\"CUDA:     {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU:      {torch.cuda.get_device_name(0)}\")\n    print(f\"VRAM:     {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\nprint(f\"Root:     {WX_AFD_ROOT}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Inspection\n",
    "\n",
    "Verify that our training data (output of `03_build_dataset.py`) is correctly formatted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonl(path):\n",
    "    \"\"\"Load JSONL file into a list of dicts.\"\"\"\n",
    "    with open(path) as f:\n",
    "        return [json.loads(line) for line in f]\n",
    "\n",
    "train_data = load_jsonl(DATA_DIR / \"train.jsonl\")\n",
    "val_data = load_jsonl(DATA_DIR / \"val.jsonl\")\n",
    "\n",
    "print(f\"Training examples:   {len(train_data)}\")\n",
    "print(f\"Validation examples: {len(val_data)}\")\n",
    "print(f\"Total:               {len(train_data) + len(val_data)}\")\n",
    "\n",
    "# Verify messages format\n",
    "ex = train_data[0]\n",
    "assert \"messages\" in ex, \"Missing 'messages' key\"\n",
    "assert len(ex[\"messages\"]) == 3, f\"Expected 3 messages, got {len(ex['messages'])}\"\n",
    "assert ex[\"messages\"][0][\"role\"] == \"system\"\n",
    "assert ex[\"messages\"][1][\"role\"] == \"user\"\n",
    "assert ex[\"messages\"][2][\"role\"] == \"assistant\"\n",
    "print(\"\\nMessages format: OK (system + user + assistant)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token length distribution\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "\n",
    "def count_tokens(example):\n",
    "    text = tokenizer.apply_chat_template(example[\"messages\"], tokenize=False)\n",
    "    return len(tokenizer.encode(text))\n",
    "\n",
    "train_lengths = [count_tokens(ex) for ex in train_data]\n",
    "val_lengths = [count_tokens(ex) for ex in val_data]\n",
    "all_lengths = train_lengths + val_lengths\n",
    "\n",
    "print(f\"Token lengths (all {len(all_lengths)} examples):\")\n",
    "print(f\"  Min:    {min(all_lengths)}\")\n",
    "print(f\"  Mean:   {sum(all_lengths) // len(all_lengths)}\")\n",
    "print(f\"  Median: {sorted(all_lengths)[len(all_lengths)//2]}\")\n",
    "print(f\"  Max:    {max(all_lengths)}\")\n",
    "print(f\"  >2048:  {sum(1 for l in all_lengths if l > 2048)} \"\n",
    "      f\"({sum(1 for l in all_lengths if l > 2048)/len(all_lengths):.1%})\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "ax.hist(all_lengths, bins=50, edgecolor=\"black\", alpha=0.7)\n",
    "ax.axvline(2048, color=\"red\", linestyle=\"--\", label=\"sequence_len=2048\")\n",
    "ax.set_xlabel(\"Total tokens per example\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "ax.set_title(\"Token Length Distribution\")\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a sample training example\n",
    "sample = train_data[0]\n",
    "print(\"=\" * 70)\n",
    "print(\"SYSTEM PROMPT:\")\n",
    "print(\"=\" * 70)\n",
    "print(sample[\"messages\"][0][\"content\"][:300], \"...\")\n",
    "print()\n",
    "print(\"=\" * 70)\n",
    "print(\"WEATHER INPUT (first 500 chars):\")\n",
    "print(\"=\" * 70)\n",
    "print(sample[\"messages\"][1][\"content\"][:500], \"...\")\n",
    "print()\n",
    "print(\"=\" * 70)\n",
    "print(\"AFD OUTPUT (first 500 chars):\")\n",
    "print(\"=\" * 70)\n",
    "print(sample[\"messages\"][2][\"content\"][:500], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Load and validate the Axolotl YAML config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import yaml\n\nwith open(CONFIG_PATH) as f:\n    config = yaml.safe_load(f)\n\n# Validate critical fields\nchecks = {\n    \"base_model\": config.get(\"base_model\") == \"Qwen/Qwen3-4B-Instruct-2507\",\n    \"chat_template\": config.get(\"chat_template\") == \"qwen3\",\n    \"eos_token\": config.get(\"special_tokens\", {}).get(\"eos_token\") == \"<|im_end|>\",\n    \"pad_token\": config.get(\"special_tokens\", {}).get(\"pad_token\") == \"<|endoftext|>\",\n    \"adapter\": config.get(\"adapter\") == \"lora\",\n    \"lora_r\": config.get(\"lora_r\") == 16,\n    \"lora_alpha\": config.get(\"lora_alpha\") == 32,\n    \"peft_use_dora\": config.get(\"peft_use_dora\") is True,\n    \"peft_use_rslora\": config.get(\"peft_use_rslora\") is True,\n    \"sample_packing\": config.get(\"sample_packing\") is True,\n    \"roles_to_train\": config[\"datasets\"][0].get(\"roles_to_train\") == [\"assistant\"],\n    \"eot_tokens\": config.get(\"eot_tokens\") == [\"<|im_end|>\"],\n    \"test_datasets\": \"test_datasets\" in config,\n    \"sequence_len\": config.get(\"sequence_len\") == 2048,\n    \"bf16\": config.get(\"bf16\") is True,\n    \"early_stopping\": config.get(\"early_stopping_patience\") == 5,\n}\n\nall_ok = True\nfor name, ok in checks.items():\n    status = \"OK\" if ok else \"FAIL\"\n    if not ok:\n        all_ok = False\n    print(f\"  [{status}] {name}\")\n\nassert all_ok, \"Config validation failed — fix issues above before training\"\nprint(\"\\nConfig validation: ALL PASSED\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sanity Check\n",
    "\n",
    "Use Axolotl's internals to validate config normalization, dataset loading, and loss masking\n",
    "**before** launching a real training job. This catches issues early without GPU time.\n",
    "\n",
    "Key checks:\n",
    "- Config normalizes without errors\n",
    "- Dataset loads and tokenizes correctly\n",
    "- Labels are `-100` for system/user tokens (loss masking)\n",
    "- EOS/PAD token IDs are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from axolotl.utils.config import normalize_config, validate_config\n",
    "from axolotl.utils.data import load_datasets\n",
    "from axolotl.utils.dict import DictDefault\n",
    "\n",
    "# Normalize and validate\n",
    "cfg = DictDefault(config)\n",
    "normalize_config(cfg)\n",
    "validate_config(cfg)\n",
    "print(\"Config normalization: OK\")\n",
    "print(\"Config validation:    OK\")\n",
    "\n",
    "# Load tokenizer for checks\n",
    "tok = AutoTokenizer.from_pretrained(cfg.base_model, trust_remote_code=True)\n",
    "eos_id = tok.convert_tokens_to_ids(\"<|im_end|>\")\n",
    "pad_id = tok.convert_tokens_to_ids(\"<|endoftext|>\")\n",
    "print(f\"EOS token ID: {eos_id} (expect 151645) — {'OK' if eos_id == 151645 else 'FAIL'}\")\n",
    "print(f\"PAD token ID: {pad_id} (expect 151643) — {'OK' if pad_id == 151643 else 'FAIL'}\")\n",
    "assert eos_id != pad_id, \"EOS and PAD must differ!\"\n",
    "\n",
    "# Load datasets (this tests the full data pipeline)\n",
    "print(\"\\nLoading datasets...\")\n",
    "train_dataset, eval_dataset, _, _ = load_datasets(cfg=cfg, cli_args={})\n",
    "print(f\"  Train: {len(train_dataset)} packed sequences\")\n",
    "if eval_dataset:\n",
    "    print(f\"  Eval:  {len(eval_dataset)} packed sequences\")\n",
    "\n",
    "# Verify loss masking: labels should be -100 for system/user tokens\n",
    "sample = train_dataset[0]\n",
    "labels = sample[\"labels\"]\n",
    "n_masked = sum(1 for l in labels if l == -100)\n",
    "n_total = len(labels)\n",
    "print(f\"\\nLoss masking (sample 0):\")\n",
    "print(f\"  Total tokens:  {n_total}\")\n",
    "print(f\"  Masked (-100): {n_masked} ({n_masked/n_total:.1%})\")\n",
    "print(f\"  Trained:       {n_total - n_masked} ({(n_total - n_masked)/n_total:.1%})\")\n",
    "assert n_masked > 0, \"No masked tokens — loss masking may be broken\"\n",
    "print(\"\\nSanity check: ALL PASSED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Training\n\nRun training directly in this notebook session (requires GPU compute node).\n\n**Expected timeline:** ~429 steps across 3 epochs, ~45-75 minutes on A100 40GB.\nEarly stopping (patience=5) may terminate after epoch 2."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INTERACTIVE MODE: Run training directly\n",
    "# Only run this cell if you're on a compute node with a GPU\n",
    "\n",
    "from axolotl.train import train\n",
    "\n",
    "train(cfg=cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Monitor training logs\nlog_dir = WX_AFD_ROOT / \"logs\"\nlogs = sorted(log_dir.glob(\"train*.log\")) if log_dir.exists() else []\nif logs:\n    latest = logs[-1]\n    print(f\"\\nLatest log: {latest}\")\n    print(\"--- Last 20 lines ---\")\n    with open(latest) as f:\n        lines = f.readlines()\n        for line in lines[-20:]:\n            print(line, end=\"\")\nelse:\n    print(\"No training logs found yet.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find trainer_state.json\n",
    "state_files = sorted(OUTPUT_DIR.glob(\"**/trainer_state.json\"))\n",
    "if not state_files:\n",
    "    print(\"No trainer_state.json found — training may not have completed yet.\")\n",
    "else:\n",
    "    state_path = state_files[-1]\n",
    "    print(f\"Loading: {state_path}\")\n",
    "    state = json.loads(state_path.read_text())\n",
    "\n",
    "    # Extract metrics\n",
    "    train_loss, train_steps = [], []\n",
    "    eval_loss, eval_steps = [], []\n",
    "    lr_values, lr_steps = [], []\n",
    "\n",
    "    for entry in state[\"log_history\"]:\n",
    "        step = entry[\"step\"]\n",
    "        if \"loss\" in entry:\n",
    "            train_loss.append(entry[\"loss\"])\n",
    "            train_steps.append(step)\n",
    "        if \"eval_loss\" in entry:\n",
    "            eval_loss.append(entry[\"eval_loss\"])\n",
    "            eval_steps.append(step)\n",
    "        if \"learning_rate\" in entry:\n",
    "            lr_values.append(entry[\"learning_rate\"])\n",
    "            lr_steps.append(step)\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    # Loss curves\n",
    "    ax1.plot(train_steps, train_loss, label=\"Train Loss\", alpha=0.7)\n",
    "    if eval_loss:\n",
    "        ax1.plot(eval_steps, eval_loss, label=\"Val Loss\", marker=\"o\", markersize=4)\n",
    "    ax1.set_xlabel(\"Step\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.set_title(\"Training & Validation Loss\")\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    # LR schedule\n",
    "    if lr_values:\n",
    "        ax2.plot(lr_steps, lr_values, color=\"green\")\n",
    "        ax2.set_xlabel(\"Step\")\n",
    "        ax2.set_ylabel(\"Learning Rate\")\n",
    "        ax2.set_title(\"Learning Rate Schedule\")\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Summary\n",
    "    print(f\"\\nTotal steps:     {state['global_step']}\")\n",
    "    print(f\"Best model step: {state.get('best_model_checkpoint', 'N/A')}\")\n",
    "    if eval_loss:\n",
    "        print(f\"Best val loss:   {min(eval_loss):.4f} (step {eval_steps[eval_loss.index(min(eval_loss))]})\")\n",
    "    print(f\"Final train loss: {train_loss[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. LoRA Merge\n",
    "\n",
    "Merge the LoRA adapter back into the base model for clean inference.\n",
    "The merged model is a standard HuggingFace directory — no adapter loading needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge LoRA adapter into base model\n",
    "merge_cmd = [\n",
    "    \"accelerate\", \"launch\", \"-m\", \"axolotl.cli.merge_lora\",\n",
    "    str(CONFIG_PATH),\n",
    "    \"--lora_model_dir\", str(OUTPUT_DIR),\n",
    "]\n",
    "print(f\"Running: {' '.join(merge_cmd)}\")\n",
    "result = subprocess.run(merge_cmd, capture_output=True, text=True)\n",
    "print(result.stdout)\n",
    "if result.returncode != 0:\n",
    "    print(f\"STDERR:\\n{result.stderr}\")\n",
    "    raise RuntimeError(\"Merge failed\")\n",
    "\n",
    "merged_dir = OUTPUT_DIR / \"merged\"\n",
    "assert merged_dir.exists(), f\"Merged model not found at {merged_dir}\"\n",
    "print(f\"\\nMerged model saved to: {merged_dir}\")\n",
    "print(f\"Contents: {[f.name for f in sorted(merged_dir.iterdir())]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy to persistent storage\n",
    "persist_dir = Path(f\"/glade/work/{os.environ['USER']}/wx-afd-model-v1\")\n",
    "persist_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "import shutil\n",
    "for item in merged_dir.iterdir():\n",
    "    dst = persist_dir / item.name\n",
    "    if item.is_file() and not dst.exists():\n",
    "        shutil.copy2(item, dst)\n",
    "    elif item.is_dir() and not dst.exists():\n",
    "        shutil.copytree(item, dst)\n",
    "\n",
    "print(f\"Merged model copied to: {persist_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Inference\n",
    "\n",
    "Load the merged model and generate AFDs. Compare against reference forecasts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from wx_afd import generate_afd, load_model\n\n# Load merged model\nmerged_dir = OUTPUT_DIR / \"merged\"\nprint(f\"Loading merged model from: {merged_dir}\")\nmodel, tokenizer = load_model(str(merged_dir))\nprint(\"Model loaded.\")\n\n# Generate from a validation example\nval_ex = val_data[0]\nweather_input = val_ex[\"messages\"][1][\"content\"]\nreference_afd = val_ex[\"messages\"][2][\"content\"]\n\ngenerated_afd = generate_afd(model, tokenizer, weather_input)\n\nprint(\"=\" * 70)\nprint(\"GENERATED AFD:\")\nprint(\"=\" * 70)\nprint(generated_afd[:1000], \"...\" if len(generated_afd) > 1000 else \"\")\nprint()\nprint(\"=\" * 70)\nprint(\"REFERENCE AFD:\")\nprint(\"=\" * 70)\nprint(reference_afd[:1000], \"...\" if len(reference_afd) > 1000 else \"\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EOS behavior check: verify model stops generating\n",
    "print(\"EOS Behavior Check — generating 5 examples...\")\n",
    "print()\n",
    "for i in range(min(5, len(val_data))):\n",
    "    inp = val_data[i][\"messages\"][1][\"content\"]\n",
    "    out = generate_afd(model, tokenizer, inp)\n",
    "    terminated = len(out) < 2048 * 4  # rough char-level check\n",
    "    print(f\"  Example {i}: {len(out)} chars — {'OK (terminated)' if terminated else 'WARNING: may not have stopped'}\")\n",
    "\n",
    "print(\"\\nAll examples should terminate cleanly with <|im_end|>.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluation\n",
    "\n",
    "Full evaluation on all validation examples using ROUGE-1/2/L, BERTScore F1, and format compliance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from rouge_score import rouge_scorer\nfrom tqdm.notebook import tqdm\nfrom wx_afd import REQUIRED_SECTIONS, compute_rouge, compute_bertscore, format_compliance\n\n# Generate all predictions\npredictions = []\nreferences = []\n\nprint(f\"Generating AFDs for {len(val_data)} validation examples...\")\nfor ex in tqdm(val_data):\n    pred = generate_afd(model, tokenizer, ex[\"messages\"][1][\"content\"])\n    predictions.append(pred)\n    references.append(ex[\"messages\"][2][\"content\"])\n\n# Save generated AFDs\nft_eval_dir = EVAL_DIR / \"finetuned\"\nft_gen_dir = ft_eval_dir / \"generated\"\nft_scores_dir = ft_eval_dir / \"scores\"\nft_gen_dir.mkdir(parents=True, exist_ok=True)\nft_scores_dir.mkdir(parents=True, exist_ok=True)\n\nfor i, pred in enumerate(predictions):\n    (ft_gen_dir / f\"example_{i:04d}.txt\").write_text(pred)\n\n# ROUGE\nrouge_avg = compute_rouge(predictions, references)\nprint(f\"\\nROUGE-1: {rouge_avg['rouge1']:.4f}\")\nprint(f\"ROUGE-2: {rouge_avg['rouge2']:.4f}\")\nprint(f\"ROUGE-L: {rouge_avg['rougeL']:.4f}\")\n\n# Per-example ROUGE (for later analysis)\nscorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\nrouge_results = {\"rouge1\": [], \"rouge2\": [], \"rougeL\": []}\nfor pred, ref in zip(predictions, references):\n    result = scorer.score(ref, pred)\n    for key in rouge_results:\n        rouge_results[key].append(result[key].fmeasure)\n\n# Free model memory before BERTScore\ndel model\ntorch.cuda.empty_cache()\n\n# BERTScore\nprint(\"\\nComputing BERTScore (this may take a minute)...\")\nbertscore_f1 = compute_bertscore(predictions, references)\nprint(f\"BERTScore F1: {bertscore_f1:.4f}\")\n\n# Format compliance\ncompliance = [format_compliance(pred)[\"compliance_score\"] for pred in predictions]\navg_compliance = sum(compliance) / len(compliance)\nprint(f\"Format compliance: {avg_compliance:.2%}\")\n\n# Save metrics\nft_metrics = {\n    \"tag\": \"finetuned\",\n    \"num_examples\": len(predictions),\n    \"rouge1\": rouge_avg[\"rouge1\"],\n    \"rouge2\": rouge_avg[\"rouge2\"],\n    \"rougeL\": rouge_avg[\"rougeL\"],\n    \"bertscore_f1\": bertscore_f1,\n    \"format_compliance\": avg_compliance,\n}\nwith open(ft_scores_dir / \"metrics.json\", \"w\") as f:\n    json.dump(ft_metrics, f, indent=2)\nprint(f\"\\nMetrics saved to {ft_scores_dir / 'metrics.json'}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Zero-shot baseline evaluation\nprint(f\"Loading zero-shot baseline: {MODEL_ID}\")\nmodel_zs, tokenizer_zs = load_model(MODEL_ID)\n\nzs_predictions = []\nprint(f\"Generating zero-shot AFDs for {len(val_data)} examples...\")\nfor ex in tqdm(val_data):\n    pred = generate_afd(model_zs, tokenizer_zs, ex[\"messages\"][1][\"content\"])\n    zs_predictions.append(pred)\n\n# Save\nzs_eval_dir = EVAL_DIR / \"zero-shot\"\nzs_gen_dir = zs_eval_dir / \"generated\"\nzs_scores_dir = zs_eval_dir / \"scores\"\nzs_gen_dir.mkdir(parents=True, exist_ok=True)\nzs_scores_dir.mkdir(parents=True, exist_ok=True)\nfor i, pred in enumerate(zs_predictions):\n    (zs_gen_dir / f\"example_{i:04d}.txt\").write_text(pred)\n\n# ROUGE\nzs_rouge_avg = compute_rouge(zs_predictions, references)\n\n# Free model before BERTScore\ndel model_zs\ntorch.cuda.empty_cache()\n\n# BERTScore\nprint(\"Computing BERTScore for zero-shot...\")\nzs_bertscore = compute_bertscore(zs_predictions, references)\n\n# Compliance\nzs_compliance = [format_compliance(pred)[\"compliance_score\"] for pred in zs_predictions]\nzs_avg_compliance = sum(zs_compliance) / len(zs_compliance)\n\nzs_metrics = {\n    \"tag\": \"zero-shot\",\n    \"num_examples\": len(zs_predictions),\n    \"rouge1\": zs_rouge_avg[\"rouge1\"],\n    \"rouge2\": zs_rouge_avg[\"rouge2\"],\n    \"rougeL\": zs_rouge_avg[\"rougeL\"],\n    \"bertscore_f1\": zs_bertscore,\n    \"format_compliance\": zs_avg_compliance,\n}\nwith open(zs_scores_dir / \"metrics.json\", \"w\") as f:\n    json.dump(zs_metrics, f, indent=2)\n\nprint(f\"\\nZero-shot ROUGE-1: {zs_rouge_avg['rouge1']:.4f}\")\nprint(f\"Zero-shot ROUGE-L: {zs_rouge_avg['rougeL']:.4f}\")\nprint(f\"Zero-shot BERTScore F1: {zs_bertscore:.4f}\")\nprint(f\"Zero-shot compliance: {zs_avg_compliance:.2%}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AFD format compliance detail\n",
    "print(\"AFD Section Presence (fine-tuned model):\")\n",
    "print()\n",
    "for sec in REQUIRED_SECTIONS:\n",
    "    present = sum(1 for p in predictions if sec in p.lower())\n",
    "    print(f\"  {sec:<15} {present}/{len(predictions)} ({present/len(predictions):.0%})\")\n",
    "\n",
    "print(\"\\nAFD Section Presence (zero-shot baseline):\")\n",
    "print()\n",
    "for sec in REQUIRED_SECTIONS:\n",
    "    present = sum(1 for p in zs_predictions if sec in p.lower())\n",
    "    print(f\"  {sec:<15} {present}/{len(zs_predictions)} ({present/len(zs_predictions):.0%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Results\n",
    "\n",
    "Compare fine-tuned model against zero-shot baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison table\n",
    "print(f\"{'Metric':<20} {'Fine-tuned':>12} {'Zero-shot':>12} {'Delta':>12}\")\n",
    "print(\"-\" * 58)\n",
    "for k in [\"rouge1\", \"rouge2\", \"rougeL\", \"bertscore_f1\", \"format_compliance\"]:\n",
    "    f_val = ft_metrics[k]\n",
    "    z_val = zs_metrics[k]\n",
    "    delta = f_val - z_val\n",
    "    sign = \"+\" if delta > 0 else \"\"\n",
    "    print(f\"{k:<20} {f_val:>12.4f} {z_val:>12.4f} {sign}{delta:>11.4f}\")\n",
    "\n",
    "# Bar chart\n",
    "import numpy as np\n",
    "\n",
    "metrics_keys = [\"rouge1\", \"rouge2\", \"rougeL\", \"bertscore_f1\", \"format_compliance\"]\n",
    "labels = [\"ROUGE-1\", \"ROUGE-2\", \"ROUGE-L\", \"BERTScore\\nF1\", \"Format\\nCompliance\"]\n",
    "ft_vals = [ft_metrics[k] for k in metrics_keys]\n",
    "zs_vals = [zs_metrics[k] for k in metrics_keys]\n",
    "\n",
    "x = np.arange(len(labels))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "bars1 = ax.bar(x - width/2, ft_vals, width, label=\"Fine-tuned\", color=\"#2196F3\")\n",
    "bars2 = ax.bar(x + width/2, zs_vals, width, label=\"Zero-shot\", color=\"#FF9800\")\n",
    "\n",
    "ax.set_ylabel(\"Score\")\n",
    "ax.set_title(\"Fine-tuned vs Zero-shot Evaluation\")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 1)\n",
    "ax.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best and worst 3 examples by ROUGE-L\n",
    "rougeL_scores = rouge_results[\"rougeL\"]\n",
    "indexed = sorted(enumerate(rougeL_scores), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TOP 3 (Best ROUGE-L):\")\n",
    "print(\"=\" * 70)\n",
    "for rank, (idx, score) in enumerate(indexed[:3], 1):\n",
    "    print(f\"\\n--- #{rank}: Example {idx} (ROUGE-L = {score:.4f}) ---\")\n",
    "    print(\"GENERATED (first 300 chars):\")\n",
    "    print(predictions[idx][:300])\n",
    "    print()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"BOTTOM 3 (Worst ROUGE-L):\")\n",
    "print(\"=\" * 70)\n",
    "for rank, (idx, score) in enumerate(indexed[-3:], 1):\n",
    "    print(f\"\\n--- #{rank}: Example {idx} (ROUGE-L = {score:.4f}) ---\")\n",
    "    print(\"GENERATED (first 300 chars):\")\n",
    "    print(predictions[idx][:300])\n",
    "    print(\"REFERENCE (first 300 chars):\")\n",
    "    print(references[idx][:300])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Next Steps\n",
    "\n",
    "**Completed in this notebook:**\n",
    "- Data inspection and validation\n",
    "- Config verification with all corrections applied\n",
    "- Sanity check (loss masking, token IDs, dataset loading)\n",
    "- Model training (DoRA + rsLoRA, 3 epochs with early stopping)\n",
    "- LoRA merge and export to persistent storage\n",
    "- Full evaluation: ROUGE, BERTScore, format compliance\n",
    "- Zero-shot baseline comparison\n",
    "\n",
    "**Next iterations (out of scope for v1):**\n",
    "- Multi-WFO training (expand beyond Louisville)\n",
    "- AlignScore factual consistency evaluation\n",
    "- GRPO/DPO alignment with AlignScore as reward signal\n",
    "- Attribute-specific LoRA adapters (synopsis vs aviation)\n",
    "- vLLM/TGI deployment for real-time inference\n",
    "\n",
    "**Model location:** `/glade/work/$USER/wx-afd-model-v1/`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}