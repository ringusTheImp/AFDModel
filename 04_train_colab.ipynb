{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# WX-AFD: Fine-Tuning Qwen3-4B for Area Forecast Discussions\n\nThis notebook fine-tunes **Qwen3-4B-Instruct-2507** with DoRA + rsLoRA to generate NWS Area Forecast Discussions from structured weather data.\n\n**Google Colab version** — requires A100 GPU + High RAM runtime.\n\n**Pipeline context:** This is step 4 in the WX-AFD pipeline:\n1. `01_scrape_afds.py` — Scrape AFDs from IEM\n2. `02_fetch_weather.py` — Fetch weather data from Open-Meteo\n3. `03_build_dataset.py` — Build training JSONL (messages format)\n4. **`04_train_colab.ipynb` — Fine-tune, evaluate, and export model** (this notebook)\n\n---\n\n**Table of Contents**\n1. [Environment Setup](#1.-Environment-Setup)\n2. [Data Inspection](#2.-Data-Inspection)\n3. [Configuration](#3.-Configuration)\n4. [Sanity Check](#4.-Sanity-Check)\n5. [Training](#5.-Training)\n6. [Training Curves](#6.-Training-Curves)\n7. [LoRA Merge](#7.-LoRA-Merge)\n8. [Inference](#8.-Inference)\n9. [Evaluation](#9.-Evaluation)\n10. [Results](#10.-Results)\n11. [Post-Training Sanity Checks](#11.-Post-Training-Sanity-Checks)\n12. [Push to HuggingFace Hub](#12.-Push-to-HuggingFace-Hub)\n13. [Next Steps](#13.-Next-Steps)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install dependencies\nimport importlib.metadata as importlib_metadata\nimport subprocess\nimport sys\n\n\ndef run(cmd, required=True):\n    print(\"$\", \" \".join(cmd))\n    rc = subprocess.call(cmd)\n    if rc != 0 and required:\n        raise RuntimeError(f\"Command failed with code {rc}: {' '.join(cmd)}\")\n    return rc == 0\n\n\nrun([sys.executable, \"-m\", \"pip\", \"install\", \"-U\", \"packaging\", \"setuptools\", \"wheel\", \"ninja\"])\nrun([\n    sys.executable,\n    \"-m\",\n    \"pip\",\n    \"install\",\n    \"-U\",\n    \"axolotl==0.14.0\",\n    \"transformers==5.0.0\",\n    \"torchvision\",\n    \"rouge-score==0.1.2\",\n    \"bert-score\",\n    \"sacrebleu\",\n    \"tqdm\",\n    \"pyyaml\",\n    \"matplotlib\",\n])\n\n# Optional acceleration package. If this fails, we disable flash_attention in config.\nflash_attn_ok = run(\n    [sys.executable, \"-m\", \"pip\", \"install\", \"flash-attn\", \"--no-build-isolation\", \"--no-cache-dir\"],\n    required=False,\n)\nif not flash_attn_ok:\n    print(\"flash-attn install failed; training will continue with flash_attention disabled.\")\n\nfor pkg in [\"torch\", \"transformers\", \"axolotl\", \"torchvision\"]:\n    try:\n        print(f\"{pkg}: {importlib_metadata.version(pkg)}\")\n    except importlib_metadata.PackageNotFoundError:\n        print(f\"{pkg}: NOT INSTALLED\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive and set up paths\n",
    "from pathlib import Path\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "DRIVE_ROOT = Path(\"/content/drive/MyDrive/wx-afd\")\n",
    "DATA_DIR = DRIVE_ROOT / \"data\"\n",
    "CONFIG_PATH = DRIVE_ROOT / \"configs\" / \"wx-afd-dora.yml\"\n",
    "OUTPUT_DIR = DRIVE_ROOT / \"output\"\n",
    "EVAL_DIR = DRIVE_ROOT / \"eval\"\n",
    "\n",
    "# Ensure directories exist\n",
    "for d in [DATA_DIR, CONFIG_PATH.parent, OUTPUT_DIR, EVAL_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Drive root: {DRIVE_ROOT}\")\n",
    "print(f\"Data dir:   {DATA_DIR}\")\n",
    "print(f\"Config:     {CONFIG_PATH}\")\n",
    "print(f\"Output:     {OUTPUT_DIR}\")\n",
    "print(f\"Eval:       {EVAL_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import shutil\nimport subprocess\nimport sys\nfrom pathlib import Path\n\nREPO_DIR = Path(\"/content/AFDModel\")\nif REPO_DIR.exists():\n    print(f\"Repo exists at {REPO_DIR}; pulling latest changes...\")\n    subprocess.check_call([\"git\", \"-C\", str(REPO_DIR), \"pull\", \"--ff-only\"])\nelse:\n    subprocess.check_call([\"git\", \"clone\", \"https://github.com/ringusTheImp/AFDModel.git\", str(REPO_DIR)])\n\nif str(REPO_DIR) not in sys.path:\n    sys.path.insert(0, str(REPO_DIR))\n\n# Check for existing data on Drive, run pipeline if missing\nif not (DATA_DIR / \"train.jsonl\").exists() or not (DATA_DIR / \"val.jsonl\").exists():\n    print(\"Data not found on Drive — running pipeline...\")\n    subprocess.check_call([sys.executable, \"01_scrape_afds.py\"], cwd=REPO_DIR)\n    subprocess.check_call([sys.executable, \"02_fetch_weather.py\"], cwd=REPO_DIR)\n    subprocess.check_call([sys.executable, \"03_build_dataset.py\"], cwd=REPO_DIR)\n    shutil.copytree(str(REPO_DIR / \"data\"), str(DATA_DIR), dirs_exist_ok=True)\n    print(f\"Data copied to Drive: {DATA_DIR}\")\nelse:\n    print(f\"Data found on Drive: {DATA_DIR}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import importlib.util\nimport yaml\n\n# Write Colab-specific Axolotl config with Drive paths\nwith open(\"/content/AFDModel/configs/wx-afd-dora.yml\") as f:\n    config_data = yaml.safe_load(f)\n\n# Patch paths for Colab + Google Drive\nconfig_data[\"datasets\"][0][\"path\"] = str(DATA_DIR / \"train.jsonl\")\nconfig_data[\"datasets\"][0][\"field_messages\"] = \"messages\"\nconfig_data[\"test_datasets\"][0][\"path\"] = str(DATA_DIR / \"val.jsonl\")\nconfig_data[\"test_datasets\"][0][\"field_messages\"] = \"messages\"\nconfig_data[\"output_dir\"] = str(OUTPUT_DIR)\n\n# Disable flash attention automatically if flash-attn is unavailable.\nhas_flash_attn = importlib.util.find_spec(\"flash_attn\") is not None\nconfig_data[\"flash_attention\"] = bool(has_flash_attn)\n\nwith open(CONFIG_PATH, \"w\") as f:\n    yaml.dump(config_data, f, default_flow_style=False, sort_keys=False)\n\nprint(f\"Config written to: {CONFIG_PATH}\")\nprint(f\"  datasets[0].path:      {config_data['datasets'][0]['path']}\")\nprint(f\"  test_datasets[0].path: {config_data['test_datasets'][0]['path']}\")\nprint(f\"  output_dir:            {config_data['output_dir']}\")\nprint(f\"  flash_attention:       {config_data['flash_attention']}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import importlib.metadata as importlib_metadata\nimport importlib.util\nimport json\nimport os\nimport subprocess\nimport sys\n\nimport matplotlib.pyplot as plt\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nMODEL_ID = \"Qwen/Qwen3-4B-Instruct-2507\"\n\n\ndef _pkg_version(name: str) -> str:\n    try:\n        return importlib_metadata.version(name)\n    except importlib_metadata.PackageNotFoundError:\n        return \"NOT INSTALLED\"\n\n\nprint(f\"PyTorch:       {torch.__version__}\")\nprint(f\"Transformers:  {_pkg_version('transformers')}\")\nprint(f\"Axolotl:       {_pkg_version('axolotl')}\")\nprint(f\"Torchvision:   {_pkg_version('torchvision')}\")\n\nif _pkg_version(\"axolotl\") == \"0.14.0\" and _pkg_version(\"transformers\") != \"5.0.0\":\n    print(\"WARNING: Axolotl 0.14.0 is pinned to transformers==5.0.0.\")\n\nif importlib.util.find_spec(\"torchvision\") is None:\n    print(\n        \"WARNING: torchvision is missing. Axolotl trainer imports can fail without it.\\n\"\n        f\"Install with: {sys.executable} -m pip install torchvision\"\n    )\n\n# ---- GPU check ----\nprint(f\"CUDA avail:    {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU:           {torch.cuda.get_device_name(0)}\")\n    print(f\"VRAM:          {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\nprint(f\"Root:          {DRIVE_ROOT}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Inspection\n",
    "\n",
    "Verify that our training data (output of `03_build_dataset.py`) is correctly formatted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def load_jsonl(path):\n    \"\"\"Load JSONL file into a list of dicts.\"\"\"\n    path = Path(path)\n    if not path.exists():\n        raise FileNotFoundError(f\"Dataset file not found: {path}\")\n    with open(path) as f:\n        return [json.loads(line) for line in f if line.strip()]\n\ntrain_data = load_jsonl(DATA_DIR / \"train.jsonl\")\nval_data = load_jsonl(DATA_DIR / \"val.jsonl\")\n\nif not train_data or not val_data:\n    raise RuntimeError(\"Train/val data is empty. Run the pipeline cells first.\")\n\nprint(f\"Training examples:   {len(train_data)}\")\nprint(f\"Validation examples: {len(val_data)}\")\nprint(f\"Total:               {len(train_data) + len(val_data)}\")\n\n# Verify messages format\nex = train_data[0]\nassert \"messages\" in ex, \"Missing 'messages' key\"\nassert len(ex[\"messages\"]) == 3, f\"Expected 3 messages, got {len(ex['messages'])}\"\nassert ex[\"messages\"][0][\"role\"] == \"system\"\nassert ex[\"messages\"][1][\"role\"] == \"user\"\nassert ex[\"messages\"][2][\"role\"] == \"assistant\"\nprint(\"\\nMessages format: OK (system + user + assistant)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Token length distribution\ntokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n\ndef count_tokens(example):\n    text = tokenizer.apply_chat_template(example[\"messages\"], tokenize=False)\n    return len(tokenizer.encode(text))\n\ntrain_lengths = [count_tokens(ex) for ex in train_data]\nval_lengths = [count_tokens(ex) for ex in val_data]\nall_lengths = train_lengths + val_lengths\n\nprint(f\"Token lengths (all {len(all_lengths)} examples):\")\nprint(f\"  Min:    {min(all_lengths)}\")\nprint(f\"  Mean:   {sum(all_lengths) // len(all_lengths)}\")\nprint(f\"  Median: {sorted(all_lengths)[len(all_lengths)//2]}\")\nprint(f\"  Max:    {max(all_lengths)}\")\nprint(f\"  >2048:  {sum(1 for l in all_lengths if l > 2048)} \"\n      f\"({sum(1 for l in all_lengths if l > 2048)/len(all_lengths):.1%})\")\n\nfig, ax = plt.subplots(figsize=(10, 4))\nax.hist(all_lengths, bins=50, edgecolor=\"black\", alpha=0.7)\nax.axvline(6144, color=\"red\", linestyle=\"--\", label=\"sequence_len=6144\")\nax.set_xlabel(\"Total tokens per example\")\nax.set_ylabel(\"Count\")\nax.set_title(\"Token Length Distribution\")\nax.legend()\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a sample training example\n",
    "sample = train_data[0]\n",
    "print(\"=\" * 70)\n",
    "print(\"SYSTEM PROMPT:\")\n",
    "print(\"=\" * 70)\n",
    "print(sample[\"messages\"][0][\"content\"][:300], \"...\")\n",
    "print()\n",
    "print(\"=\" * 70)\n",
    "print(\"WEATHER INPUT (first 500 chars):\")\n",
    "print(\"=\" * 70)\n",
    "print(sample[\"messages\"][1][\"content\"][:500], \"...\")\n",
    "print()\n",
    "print(\"=\" * 70)\n",
    "print(\"AFD OUTPUT (first 500 chars):\")\n",
    "print(\"=\" * 70)\n",
    "print(sample[\"messages\"][2][\"content\"][:500], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuration\n",
    "\n",
    "Load and validate the Axolotl YAML config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "with open(CONFIG_PATH) as f:\n    config = yaml.safe_load(f)\n\ntrain_ds = config.get(\"datasets\", [{}])[0] if config.get(\"datasets\") else {}\nval_ds = config.get(\"test_datasets\", [{}])[0] if config.get(\"test_datasets\") else {}\nsequence_len = config.get(\"sequence_len\")\n\nchecks = {\n    \"base_model\": config.get(\"base_model\") == \"Qwen/Qwen3-4B-Instruct-2507\",\n    \"chat_template\": config.get(\"chat_template\") in {\"tokenizer_default\", \"qwen3\"},\n    \"eos_token\": config.get(\"special_tokens\", {}).get(\"eos_token\") == \"<|im_end|>\",\n    \"pad_token\": config.get(\"special_tokens\", {}).get(\"pad_token\") == \"<|endoftext|>\",\n    \"eos_pad_distinct\": config.get(\"special_tokens\", {}).get(\"eos_token\") != config.get(\"special_tokens\", {}).get(\"pad_token\"),\n    \"adapter\": config.get(\"adapter\") == \"lora\",\n    \"lora_r\": config.get(\"lora_r\") == 16,\n    \"lora_alpha\": config.get(\"lora_alpha\") == 32,\n    \"peft_use_dora\": config.get(\"peft_use_dora\") is True,\n    \"peft_use_rslora\": config.get(\"peft_use_rslora\") is True,\n    \"sample_packing\": config.get(\"sample_packing\") is True,\n    \"train_roles_to_train\": train_ds.get(\"roles_to_train\") == [\"assistant\"],\n    \"train_field_messages\": train_ds.get(\"field_messages\") == \"messages\",\n    \"train_on_eos\": train_ds.get(\"train_on_eos\") in {\"all\", \"turn\", \"last\"},\n    \"test_datasets\": \"test_datasets\" in config,\n    \"val_field_messages\": val_ds.get(\"field_messages\") == \"messages\",\n    \"eot_tokens\": config.get(\"eot_tokens\") == [\"<|im_end|>\"],\n    \"sequence_len>=2048\": isinstance(sequence_len, int) and sequence_len >= 2048,\n    \"bf16\": config.get(\"bf16\") is True,\n    \"early_stopping\": config.get(\"early_stopping_patience\") == 5,\n    \"batch_key_xor\": (\"batch_size\" in config) ^ (\"gradient_accumulation_steps\" in config),\n}\n\nif \"all_lengths\" in globals() and all_lengths and isinstance(sequence_len, int):\n    coverage = sum(1 for n in all_lengths if n <= sequence_len) / len(all_lengths)\n    checks[\"sequence_len_coverage>=95%\"] = coverage >= 0.95\n    print(f\"Sequence coverage @ {sequence_len} tokens: {coverage:.1%}\")\n\nall_ok = True\nfor name, ok in checks.items():\n    status = \"OK\" if ok else \"FAIL\"\n    if not ok:\n        all_ok = False\n    print(f\"  [{status}] {name}\")\n\nassert all_ok, \"Config validation failed — fix issues above before training\"\nprint(\"\\nConfig validation: ALL PASSED\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sanity Check\n",
    "\n",
    "Use Axolotl's internals to validate config normalization, dataset loading, and loss masking\n",
    "**before** launching a real training job. This catches issues early without GPU time.\n",
    "\n",
    "Key checks:\n",
    "- Config normalizes without errors\n",
    "- Dataset loads and tokenizes correctly\n",
    "- Labels are `-100` for system/user tokens (loss masking)\n",
    "- EOS/PAD token IDs are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from axolotl.utils.config import normalize_config, validate_config\nfrom axolotl.common.datasets import load_datasets\nfrom axolotl.utils.dict import DictDefault\n\n# Workaround: Axolotl 0.14.0 bug — SFTDataset pydantic model not subscriptable\nfrom axolotl.utils.schemas.config import SFTDataset\nif not hasattr(SFTDataset, \"__getitem__\"):\n    SFTDataset.__getitem__ = lambda self, key: getattr(self, key)\n\n\ndef assert_trainer_import_path():\n    \"\"\"Fail fast on optional dependency issues before model weight loading.\"\"\"\n    try:\n        from axolotl.core.builders import HFCausalTrainerBuilder  # noqa: F401\n    except ModuleNotFoundError as exc:\n        if \"torchvision\" in str(exc):\n            raise ModuleNotFoundError(\n                \"Missing dependency 'torchvision'. This is required by the \"\n                \"Axolotl 0.14.0 + transformers 5.x trainer import path.\\n\"\n                f\"Install with: {sys.executable} -m pip install torchvision\"\n            ) from exc\n        raise\n\n\n# Normalize and validate\ncfg = DictDefault(config)\ncfg = validate_config(cfg)\ncfg_norm = normalize_config(cfg)\nif cfg_norm is not None:\n    cfg = cfg_norm\nprint(\"Config validation:    OK\")\nprint(\"Config normalization: OK\")\n\n# Fail fast before loading model weights.\nassert_trainer_import_path()\nprint(\"Trainer import path:  OK\")\n\n# Load tokenizer for checks\ntok = AutoTokenizer.from_pretrained(cfg.base_model, trust_remote_code=True)\neos_id = tok.convert_tokens_to_ids(\"<|im_end|>\")\npad_id = tok.convert_tokens_to_ids(\"<|endoftext|>\")\nprint(f\"EOS token ID: {eos_id} (expect 151645) — {'OK' if eos_id == 151645 else 'FAIL'}\")\nprint(f\"PAD token ID: {pad_id} (expect 151643) — {'OK' if pad_id == 151643 else 'FAIL'}\")\nassert eos_id != pad_id, \"EOS and PAD must differ!\"\n\n# Load datasets (this tests the full data pipeline)\nprint(\"\\nLoading datasets...\")\ndataset_meta = load_datasets(cfg=cfg)\ntrain_dataset = dataset_meta.train_dataset\neval_dataset = dataset_meta.eval_dataset\nprint(f\"  Train: {len(train_dataset)} packed sequences\")\nif eval_dataset:\n    print(f\"  Eval:  {len(eval_dataset)} packed sequences\")\n\n# Verify loss masking: labels should be -100 for system/user tokens\nsample = train_dataset[0]\nlabels = sample[\"labels\"]\nn_masked = sum(1 for l in labels if l == -100)\nn_total = len(labels)\nprint(f\"\\nLoss masking (sample 0):\")\nprint(f\"  Total tokens:  {n_total}\")\nprint(f\"  Masked (-100): {n_masked} ({n_masked/n_total:.1%})\")\nprint(f\"  Trained:       {n_total - n_masked} ({(n_total - n_masked)/n_total:.1%})\")\nassert n_masked > 0, \"No masked tokens — loss masking may be broken\"\nprint(\"\\nSanity check: ALL PASSED\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training\n",
    "\n",
    "Run training directly in this notebook session (requires A100 GPU runtime).\n",
    "\n",
    "**Expected timeline:** ~429 steps across 3 epochs, ~45-75 minutes on A100 40GB.\n",
    "Early stopping (patience=5) may terminate after epoch 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# INTERACTIVE MODE: Run training directly\nif not torch.cuda.is_available():\n    raise RuntimeError(\"CUDA GPU is required for training. Switch Colab runtime to GPU.\")\n\nif \"cfg\" not in globals() or \"dataset_meta\" not in globals():\n    raise RuntimeError(\"Run the Sanity Check cell first to build `cfg` and `dataset_meta`.\")\n\nif \"assert_trainer_import_path\" in globals():\n    assert_trainer_import_path()\n\nfrom axolotl.train import train\n\ntrain(cfg=cfg, dataset_meta=dataset_meta)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor training logs\n",
    "log_dir = OUTPUT_DIR / \"runs\"\n",
    "logs = sorted(log_dir.glob(\"**/events.*\")) if log_dir.exists() else []\n",
    "if logs:\n",
    "    print(f\"TensorBoard logs found: {len(logs)} event files\")\n",
    "    print(f\"  Directory: {log_dir}\")\n",
    "else:\n",
    "    print(\"No TensorBoard logs found yet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find trainer_state.json\n",
    "state_files = sorted(OUTPUT_DIR.glob(\"**/trainer_state.json\"))\n",
    "if not state_files:\n",
    "    print(\"No trainer_state.json found — training may not have completed yet.\")\n",
    "else:\n",
    "    state_path = state_files[-1]\n",
    "    print(f\"Loading: {state_path}\")\n",
    "    state = json.loads(state_path.read_text())\n",
    "\n",
    "    # Extract metrics\n",
    "    train_loss, train_steps = [], []\n",
    "    eval_loss, eval_steps = [], []\n",
    "    lr_values, lr_steps = [], []\n",
    "\n",
    "    for entry in state[\"log_history\"]:\n",
    "        step = entry[\"step\"]\n",
    "        if \"loss\" in entry:\n",
    "            train_loss.append(entry[\"loss\"])\n",
    "            train_steps.append(step)\n",
    "        if \"eval_loss\" in entry:\n",
    "            eval_loss.append(entry[\"eval_loss\"])\n",
    "            eval_steps.append(step)\n",
    "        if \"learning_rate\" in entry:\n",
    "            lr_values.append(entry[\"learning_rate\"])\n",
    "            lr_steps.append(step)\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    # Loss curves\n",
    "    ax1.plot(train_steps, train_loss, label=\"Train Loss\", alpha=0.7)\n",
    "    if eval_loss:\n",
    "        ax1.plot(eval_steps, eval_loss, label=\"Val Loss\", marker=\"o\", markersize=4)\n",
    "    ax1.set_xlabel(\"Step\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.set_title(\"Training & Validation Loss\")\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    # LR schedule\n",
    "    if lr_values:\n",
    "        ax2.plot(lr_steps, lr_values, color=\"green\")\n",
    "        ax2.set_xlabel(\"Step\")\n",
    "        ax2.set_ylabel(\"Learning Rate\")\n",
    "        ax2.set_title(\"Learning Rate Schedule\")\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Summary\n",
    "    print(f\"\\nTotal steps:     {state['global_step']}\")\n",
    "    print(f\"Best model step: {state.get('best_model_checkpoint', 'N/A')}\")\n",
    "    if eval_loss:\n",
    "        print(f\"Best val loss:   {min(eval_loss):.4f} (step {eval_steps[eval_loss.index(min(eval_loss))]})\")\n",
    "    print(f\"Final train loss: {train_loss[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 6. Training Curves",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7. LoRA Merge\n\nMerge the LoRA adapter back into the base model for clean inference.\nThe merged model is a standard HuggingFace directory — no adapter loading needed."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Merge LoRA adapter into base model\nif \"cfg\" not in globals():\n    raise RuntimeError(\"Run the Sanity Check cell first to build `cfg`.\")\n\nmerge_cmd = [\n    sys.executable,\n    \"-m\",\n    \"accelerate.commands.launch\",\n    \"-m\",\n    \"axolotl.cli.merge_lora\",\n    str(CONFIG_PATH),\n    \"--lora_model_dir\",\n    str(OUTPUT_DIR),\n]\nprint(f\"Running: {' '.join(merge_cmd)}\")\nresult = subprocess.run(merge_cmd, capture_output=True, text=True)\nprint(result.stdout)\nif result.returncode != 0:\n    print(f\"STDERR:\\\\n{result.stderr}\")\n    raise RuntimeError(\"Merge failed\")\n\nmerged_dir = OUTPUT_DIR / \"merged\"\nassert merged_dir.exists(), f\"Merged model not found at {merged_dir}\"\nprint(f\"\\nMerged model saved to: {merged_dir}\")\nprint(f\"Contents: {[f.name for f in sorted(merged_dir.iterdir())]}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merged model is already on Google Drive — no copy needed\n",
    "print(f\"Merged model saved to Google Drive: {OUTPUT_DIR / 'merged'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Inference\n",
    "\n",
    "Load the merged model and generate AFDs. Compare against reference forecasts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from wx_afd import generate_afd, load_model\n\n# Load merged model\nmerged_dir = OUTPUT_DIR / \"merged\"\nif not merged_dir.exists():\n    raise RuntimeError(\"Merged model not found. Run the LoRA Merge cell first.\")\n\nprint(f\"Loading merged model from: {merged_dir}\")\nmodel, tokenizer = load_model(str(merged_dir))\nprint(\"Model loaded.\")\n\n# Generate from a validation example\nval_ex = val_data[0]\nweather_input = val_ex[\"messages\"][1][\"content\"]\nreference_afd = val_ex[\"messages\"][2][\"content\"]\n\ngenerated_afd = generate_afd(model, tokenizer, weather_input)\n\nprint(\"=\" * 70)\nprint(\"GENERATED AFD:\")\nprint(\"=\" * 70)\nprint(generated_afd[:1000], \"...\" if len(generated_afd) > 1000 else \"\")\nprint()\nprint(\"=\" * 70)\nprint(\"REFERENCE AFD:\")\nprint(\"=\" * 70)\nprint(reference_afd[:1000], \"...\" if len(reference_afd) > 1000 else \"\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EOS behavior check: verify model stops generating\n",
    "print(\"EOS Behavior Check — generating 5 examples...\")\n",
    "print()\n",
    "for i in range(min(5, len(val_data))):\n",
    "    inp = val_data[i][\"messages\"][1][\"content\"]\n",
    "    out = generate_afd(model, tokenizer, inp)\n",
    "    terminated = len(out) < 2048 * 4  # rough char-level check\n",
    "    print(f\"  Example {i}: {len(out)} chars — {'OK (terminated)' if terminated else 'WARNING: may not have stopped'}\")\n",
    "\n",
    "print(\"\\nAll examples should terminate cleanly with <|im_end|>.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Evaluation\n",
    "\n",
    "Full evaluation on all validation examples using ROUGE-1/2/L, BERTScore F1, and format compliance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from rouge_score import rouge_scorer\nfrom tqdm.auto import tqdm\nfrom wx_afd import REQUIRED_SECTIONS, compute_rouge, compute_bertscore, format_compliance\n\n# Generate all predictions\npredictions = []\nreferences = []\n\nprint(f\"Generating AFDs for {len(val_data)} validation examples...\")\nfor ex in tqdm(val_data):\n    pred = generate_afd(model, tokenizer, ex[\"messages\"][1][\"content\"])\n    predictions.append(pred)\n    references.append(ex[\"messages\"][2][\"content\"])\n\n# Save generated AFDs\nft_eval_dir = EVAL_DIR / \"finetuned\"\nft_gen_dir = ft_eval_dir / \"generated\"\nft_scores_dir = ft_eval_dir / \"scores\"\nft_gen_dir.mkdir(parents=True, exist_ok=True)\nft_scores_dir.mkdir(parents=True, exist_ok=True)\n\nfor i, pred in enumerate(predictions):\n    (ft_gen_dir / f\"example_{i:04d}.txt\").write_text(pred)\n\n# ROUGE\nrouge_avg = compute_rouge(predictions, references)\nprint(f\"\\nROUGE-1: {rouge_avg['rouge1']:.4f}\")\nprint(f\"ROUGE-2: {rouge_avg['rouge2']:.4f}\")\nprint(f\"ROUGE-L: {rouge_avg['rougeL']:.4f}\")\n\n# Per-example ROUGE (for later analysis)\nscorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\nrouge_results = {\"rouge1\": [], \"rouge2\": [], \"rougeL\": []}\nfor pred, ref in zip(predictions, references):\n    result = scorer.score(ref, pred)\n    for key in rouge_results:\n        rouge_results[key].append(result[key].fmeasure)\n\n# Free model memory before BERTScore\ndel model\ntorch.cuda.empty_cache()\n\n# BERTScore\nprint(\"\\nComputing BERTScore (this may take a minute)...\")\nbertscore_f1 = compute_bertscore(predictions, references)\nprint(f\"BERTScore F1: {bertscore_f1:.4f}\")\n\n# Format compliance\ncompliance = [format_compliance(pred)[\"compliance_score\"] for pred in predictions]\navg_compliance = sum(compliance) / len(compliance)\nprint(f\"Format compliance: {avg_compliance:.2%}\")\n\n# Save metrics\nft_metrics = {\n    \"tag\": \"finetuned\",\n    \"num_examples\": len(predictions),\n    \"rouge1\": rouge_avg[\"rouge1\"],\n    \"rouge2\": rouge_avg[\"rouge2\"],\n    \"rougeL\": rouge_avg[\"rougeL\"],\n    \"bertscore_f1\": bertscore_f1,\n    \"format_compliance\": avg_compliance,\n}\nwith open(ft_scores_dir / \"metrics.json\", \"w\") as f:\n    json.dump(ft_metrics, f, indent=2)\nprint(f\"\\nMetrics saved to {ft_scores_dir / 'metrics.json'}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero-shot baseline evaluation\n",
    "print(f\"Loading zero-shot baseline: {MODEL_ID}\")\n",
    "model_zs, tokenizer_zs = load_model(MODEL_ID)\n",
    "\n",
    "zs_predictions = []\n",
    "print(f\"Generating zero-shot AFDs for {len(val_data)} examples...\")\n",
    "for ex in tqdm(val_data):\n",
    "    pred = generate_afd(model_zs, tokenizer_zs, ex[\"messages\"][1][\"content\"])\n",
    "    zs_predictions.append(pred)\n",
    "\n",
    "# Save\n",
    "zs_eval_dir = EVAL_DIR / \"zero-shot\"\n",
    "zs_gen_dir = zs_eval_dir / \"generated\"\n",
    "zs_scores_dir = zs_eval_dir / \"scores\"\n",
    "zs_gen_dir.mkdir(parents=True, exist_ok=True)\n",
    "zs_scores_dir.mkdir(parents=True, exist_ok=True)\n",
    "for i, pred in enumerate(zs_predictions):\n",
    "    (zs_gen_dir / f\"example_{i:04d}.txt\").write_text(pred)\n",
    "\n",
    "# ROUGE\n",
    "zs_rouge_avg = compute_rouge(zs_predictions, references)\n",
    "\n",
    "# Free model before BERTScore\n",
    "del model_zs\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# BERTScore\n",
    "print(\"Computing BERTScore for zero-shot...\")\n",
    "zs_bertscore = compute_bertscore(zs_predictions, references)\n",
    "\n",
    "# Compliance\n",
    "zs_compliance = [format_compliance(pred)[\"compliance_score\"] for pred in zs_predictions]\n",
    "zs_avg_compliance = sum(zs_compliance) / len(zs_compliance)\n",
    "\n",
    "zs_metrics = {\n",
    "    \"tag\": \"zero-shot\",\n",
    "    \"num_examples\": len(zs_predictions),\n",
    "    \"rouge1\": zs_rouge_avg[\"rouge1\"],\n",
    "    \"rouge2\": zs_rouge_avg[\"rouge2\"],\n",
    "    \"rougeL\": zs_rouge_avg[\"rougeL\"],\n",
    "    \"bertscore_f1\": zs_bertscore,\n",
    "    \"format_compliance\": zs_avg_compliance,\n",
    "}\n",
    "with open(zs_scores_dir / \"metrics.json\", \"w\") as f:\n",
    "    json.dump(zs_metrics, f, indent=2)\n",
    "\n",
    "print(f\"\\nZero-shot ROUGE-1: {zs_rouge_avg['rouge1']:.4f}\")\n",
    "print(f\"Zero-shot ROUGE-L: {zs_rouge_avg['rougeL']:.4f}\")\n",
    "print(f\"Zero-shot BERTScore F1: {zs_bertscore:.4f}\")\n",
    "print(f\"Zero-shot compliance: {zs_avg_compliance:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AFD format compliance detail\n",
    "print(\"AFD Section Presence (fine-tuned model):\")\n",
    "print()\n",
    "for sec in REQUIRED_SECTIONS:\n",
    "    present = sum(1 for p in predictions if sec in p.lower())\n",
    "    print(f\"  {sec:<15} {present}/{len(predictions)} ({present/len(predictions):.0%})\")\n",
    "\n",
    "print(\"\\nAFD Section Presence (zero-shot baseline):\")\n",
    "print()\n",
    "for sec in REQUIRED_SECTIONS:\n",
    "    present = sum(1 for p in zs_predictions if sec in p.lower())\n",
    "    print(f\"  {sec:<15} {present}/{len(zs_predictions)} ({present/len(zs_predictions):.0%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Results\n",
    "\n",
    "Compare fine-tuned model against zero-shot baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison table\n",
    "print(f\"{'Metric':<20} {'Fine-tuned':>12} {'Zero-shot':>12} {'Delta':>12}\")\n",
    "print(\"-\" * 58)\n",
    "for k in [\"rouge1\", \"rouge2\", \"rougeL\", \"bertscore_f1\", \"format_compliance\"]:\n",
    "    f_val = ft_metrics[k]\n",
    "    z_val = zs_metrics[k]\n",
    "    delta = f_val - z_val\n",
    "    sign = \"+\" if delta > 0 else \"\"\n",
    "    print(f\"{k:<20} {f_val:>12.4f} {z_val:>12.4f} {sign}{delta:>11.4f}\")\n",
    "\n",
    "# Bar chart\n",
    "import numpy as np\n",
    "\n",
    "metrics_keys = [\"rouge1\", \"rouge2\", \"rougeL\", \"bertscore_f1\", \"format_compliance\"]\n",
    "labels = [\"ROUGE-1\", \"ROUGE-2\", \"ROUGE-L\", \"BERTScore\\nF1\", \"Format\\nCompliance\"]\n",
    "ft_vals = [ft_metrics[k] for k in metrics_keys]\n",
    "zs_vals = [zs_metrics[k] for k in metrics_keys]\n",
    "\n",
    "x = np.arange(len(labels))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "bars1 = ax.bar(x - width/2, ft_vals, width, label=\"Fine-tuned\", color=\"#2196F3\")\n",
    "bars2 = ax.bar(x + width/2, zs_vals, width, label=\"Zero-shot\", color=\"#FF9800\")\n",
    "\n",
    "ax.set_ylabel(\"Score\")\n",
    "ax.set_title(\"Fine-tuned vs Zero-shot Evaluation\")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 1)\n",
    "ax.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best and worst 3 examples by ROUGE-L\n",
    "rougeL_scores = rouge_results[\"rougeL\"]\n",
    "indexed = sorted(enumerate(rougeL_scores), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TOP 3 (Best ROUGE-L):\")\n",
    "print(\"=\" * 70)\n",
    "for rank, (idx, score) in enumerate(indexed[:3], 1):\n",
    "    print(f\"\\n--- #{rank}: Example {idx} (ROUGE-L = {score:.4f}) ---\")\n",
    "    print(\"GENERATED (first 300 chars):\")\n",
    "    print(predictions[idx][:300])\n",
    "    print()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"BOTTOM 3 (Worst ROUGE-L):\")\n",
    "print(\"=\" * 70)\n",
    "for rank, (idx, score) in enumerate(indexed[-3:], 1):\n",
    "    print(f\"\\n--- #{rank}: Example {idx} (ROUGE-L = {score:.4f}) ---\")\n",
    "    print(\"GENERATED (first 300 chars):\")\n",
    "    print(predictions[idx][:300])\n",
    "    print(\"REFERENCE (first 300 chars):\")\n",
    "    print(references[idx][:300])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 11. Post-Training Sanity Checks\n\nSix gates that must **all pass** before publishing the model. These verify that the\nmerged model is structurally complete, generates properly terminated output, and\nmeets minimum quality thresholds relative to both absolute baselines and the\nzero-shot model.\n\n| Gate | Check |\n|------|-------|\n| 1 | Merged model directory contains required files |\n| 2 | Generated outputs terminate before `max_new_tokens` |\n| 3 | ROUGE-L > 0.15 |\n| 4 | BERTScore F1 > 0.50 |\n| 5 | Format compliance > 50% |\n| 6 | Fine-tuned metrics beat zero-shot baseline |",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Gate 1: Merged model directory check\nprint(\"Gate 1: Merged model directory ...\")\nmerged_dir = OUTPUT_DIR / \"merged\"\nassert merged_dir.exists(), f\"Merged dir not found: {merged_dir}\"\n\nrequired_files = [\"config.json\", \"tokenizer_config.json\"]\nfor fname in required_files:\n    assert (merged_dir / fname).exists(), f\"Missing {fname} in merged dir\"\n\nif not ((merged_dir / \"tokenizer.json\").exists() or (merged_dir / \"tokenizer.model\").exists()):\n    raise AssertionError(\"Missing tokenizer artifact (tokenizer.json or tokenizer.model)\")\n\nsafetensors = list(merged_dir.glob(\"*.safetensors\"))\nassert len(safetensors) >= 1, \"No .safetensors files in merged dir\"\n\nprint(f\"  Directory: {merged_dir}\")\nprint(f\"  Required files: {required_files} + tokenizer artifact — all present\")\nprint(f\"  Safetensors shards: {len(safetensors)}\")\nprint(\"Gate 1: PASSED\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gate 2: EOS termination check\n",
    "# Model was del'd during eval for BERTScore memory — reload it\n",
    "print(\"Gate 2: EOS termination ...\")\n",
    "model, tokenizer = load_model(str(merged_dir))\n",
    "\n",
    "MAX_NEW_TOKENS = 2048\n",
    "MARGIN = 10\n",
    "test_examples = val_data[:3]\n",
    "\n",
    "for i, ex in enumerate(test_examples):\n",
    "    weather_input = ex[\"messages\"][1][\"content\"]\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert NWS meteorologist.\"},\n",
    "        {\"role\": \"user\", \"content\": weather_input},\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=MAX_NEW_TOKENS,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            eos_token_id=tokenizer.convert_tokens_to_ids(\"<|im_end|>\"),\n",
    "            pad_token_id=tokenizer.convert_tokens_to_ids(\"<|endoftext|>\"),\n",
    "        )\n",
    "    generated_tokens = output[0][inputs[\"input_ids\"].shape[1]:]\n",
    "    n_gen = len(generated_tokens)\n",
    "    assert n_gen < MAX_NEW_TOKENS - MARGIN, (\n",
    "        f\"Example {i}: generated {n_gen} tokens (limit {MAX_NEW_TOKENS}), \"\n",
    "        f\"model may not be terminating with EOS\"\n",
    "    )\n",
    "    print(f\"  Example {i}: {n_gen} tokens — OK\")\n",
    "\n",
    "# Free model again\n",
    "del model\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Gate 2: PASSED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gates 3-6: Metric thresholds\n",
    "print(\"Gate 3: ROUGE-L > 0.15 ...\")\n",
    "assert ft_metrics[\"rougeL\"] > 0.15, (\n",
    "    f\"ROUGE-L = {ft_metrics['rougeL']:.4f} (threshold: 0.15)\"\n",
    ")\n",
    "print(f\"  ROUGE-L = {ft_metrics['rougeL']:.4f} — PASSED\")\n",
    "\n",
    "print(\"Gate 4: BERTScore F1 > 0.50 ...\")\n",
    "assert ft_metrics[\"bertscore_f1\"] > 0.50, (\n",
    "    f\"BERTScore F1 = {ft_metrics['bertscore_f1']:.4f} (threshold: 0.50)\"\n",
    ")\n",
    "print(f\"  BERTScore F1 = {ft_metrics['bertscore_f1']:.4f} — PASSED\")\n",
    "\n",
    "print(\"Gate 5: Format compliance > 0.50 ...\")\n",
    "assert ft_metrics[\"format_compliance\"] > 0.50, (\n",
    "    f\"Format compliance = {ft_metrics['format_compliance']:.4f} (threshold: 0.50)\"\n",
    ")\n",
    "print(f\"  Format compliance = {ft_metrics['format_compliance']:.4f} — PASSED\")\n",
    "\n",
    "print(\"Gate 6: Fine-tuned > zero-shot ...\")\n",
    "assert ft_metrics[\"rougeL\"] > zs_metrics[\"rougeL\"], (\n",
    "    f\"ROUGE-L: fine-tuned ({ft_metrics['rougeL']:.4f}) <= \"\n",
    "    f\"zero-shot ({zs_metrics['rougeL']:.4f})\"\n",
    ")\n",
    "assert ft_metrics[\"bertscore_f1\"] > zs_metrics[\"bertscore_f1\"], (\n",
    "    f\"BERTScore F1: fine-tuned ({ft_metrics['bertscore_f1']:.4f}) <= \"\n",
    "    f\"zero-shot ({zs_metrics['bertscore_f1']:.4f})\"\n",
    ")\n",
    "print(f\"  ROUGE-L:     {ft_metrics['rougeL']:.4f} > {zs_metrics['rougeL']:.4f} — PASSED\")\n",
    "print(f\"  BERTScore:   {ft_metrics['bertscore_f1']:.4f} > {zs_metrics['bertscore_f1']:.4f} — PASSED\")\n",
    "\n",
    "print()\n",
    "print(\"=\" * 60)\n",
    "print(\"ALL SANITY CHECKS PASSED — safe to push to HF Hub\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Push to HuggingFace Hub\n",
    "\n",
    "Upload the merged model and GGUF quantizations to HuggingFace:\n",
    "\n",
    "| Repo | Contents |\n",
    "|------|----------|\n",
    "| `ringusTheImp/wx-afd-qwen3-4b` | Merged HF model + tokenizer |\n",
    "| `ringusTheImp/wx-afd-qwen3-4b-GGUF` | F16, Q8_0, Q4_K_M quantizations |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "!pip install -U huggingface_hub\n\nimport os\nfrom getpass import getpass\n\nfrom huggingface_hub import HfApi, login\n\nHF_TOKEN = os.environ.get(\"HF_TOKEN\", \"\").strip()\nif not HF_TOKEN:\n    HF_TOKEN = getpass(\"Enter your HF token (starts with hf_): \").strip()\nif not HF_TOKEN.startswith(\"hf_\"):\n    raise ValueError(\"Invalid HF token. Set HF_TOKEN in the environment or enter a valid token.\")\n\nlogin(token=HF_TOKEN)\napi = HfApi()\nwhoami = api.whoami()\nprint(f\"Logged in as: {whoami['name']}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Push merged model to HuggingFace Hub\nfrom huggingface_hub import ModelCard\n\nHF_REPO = \"ringusTheImp/wx-afd-qwen3-4b\"\nGGUF_REPO = \"ringusTheImp/wx-afd-qwen3-4b-GGUF\"\n\n# Build model card\nmodel_card_text = f\"\"\"---\nlicense: apache-2.0\nbase_model: Qwen/Qwen3-4B-Instruct-2507\ntags:\n  - weather\n  - meteorology\n  - nws\n  - area-forecast-discussion\n  - dora\n  - rslora\n  - axolotl\ndatasets:\n  - custom\nlanguage:\n  - en\n---\n\n# WX-AFD: Qwen3-4B for NWS Area Forecast Discussions\n\nFine-tuned **Qwen3-4B-Instruct-2507** with DoRA + rsLoRA to generate\nNWS Area Forecast Discussions (AFDs) from structured weather model data.\nTrained on Louisville, KY (WFO LMK) forecasts.\n\n## Evaluation Results\n\n| Metric | Fine-tuned | Zero-shot | Delta |\n|--------|-----------|-----------|-------|\n| ROUGE-1 | {ft_metrics['rouge1']:.4f} | {zs_metrics['rouge1']:.4f} | {ft_metrics['rouge1'] - zs_metrics['rouge1']:+.4f} |\n| ROUGE-2 | {ft_metrics['rouge2']:.4f} | {zs_metrics['rouge2']:.4f} | {ft_metrics['rouge2'] - zs_metrics['rouge2']:+.4f} |\n| ROUGE-L | {ft_metrics['rougeL']:.4f} | {zs_metrics['rougeL']:.4f} | {ft_metrics['rougeL'] - zs_metrics['rougeL']:+.4f} |\n| BERTScore F1 | {ft_metrics['bertscore_f1']:.4f} | {zs_metrics['bertscore_f1']:.4f} | {ft_metrics['bertscore_f1'] - zs_metrics['bertscore_f1']:+.4f} |\n| Format Compliance | {ft_metrics['format_compliance']:.2%} | {zs_metrics['format_compliance']:.2%} | {ft_metrics['format_compliance'] - zs_metrics['format_compliance']:+.2%} |\n\n## Usage\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"{HF_REPO}\",\n    torch_dtype=torch.bfloat16,\n    device_map=\"auto\",\n)\ntokenizer = AutoTokenizer.from_pretrained(\"{HF_REPO}\")\n\nmessages = [\n    {{\"role\": \"system\", \"content\": \"You are an expert NWS meteorologist at the Louisville, Kentucky Weather Forecast Office (WFO LMK).\"}},\n    {{\"role\": \"user\", \"content\": \"<your weather data here>\"}},\n]\ntext = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\ninputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\noutput = model.generate(**inputs, max_new_tokens=2048, temperature=0.7)\nprint(tokenizer.decode(output[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True))\n```\n\n## GGUF Quantizations\n\nGGUF files for llama.cpp / Ollama / LM Studio are available at\n[{GGUF_REPO}](https://huggingface.co/{GGUF_REPO}):\n- `wx-afd-qwen3-4b-F16.gguf` — Full FP16\n- `wx-afd-qwen3-4b-Q8_0.gguf` — 8-bit quantization\n- `wx-afd-qwen3-4b-Q4_K_M.gguf` — 4-bit quantization (recommended for CPU)\n\n## MLX (Apple Silicon)\n\nMLX conversion requires macOS with Apple Silicon — run locally:\n\n```bash\npip install mlx-lm\nmlx_lm.convert --hf-path {HF_REPO} -q --upload-repo ringusTheImp/wx-afd-qwen3-4b-MLX\n```\n\n## Training Details\n\n- **Base model:** Qwen/Qwen3-4B-Instruct-2507\n- **Method:** DoRA + rsLoRA (rank 16, alpha 32)\n- **Framework:** Axolotl\n- **Data:** {ft_metrics['num_examples']} validation examples from Louisville, KY WFO (LMK)\n- **Sequence length:** 2048 tokens (sample packing)\n- **Precision:** bfloat16\n\n## Citation\n\n```bibtex\n@misc{{wx-afd-qwen3-4b,\n  title={{WX-AFD: Fine-Tuning Qwen3-4B for Area Forecast Discussions}},\n  author={{ringusTheImp}},\n  year={{2026}},\n  url={{https://huggingface.co/{HF_REPO}}}\n}}\n```\n\"\"\"\n\n# Create repo and upload model\napi.create_repo(HF_REPO, exist_ok=True)\nprint(f\"Uploading merged model to {HF_REPO} ...\")\napi.upload_folder(\n    folder_path=str(merged_dir),\n    repo_id=HF_REPO,\n    commit_message=\"Upload merged WX-AFD model\",\n)\nprint(\"Model uploaded.\")\n\n# Push model card\ncard = ModelCard(model_card_text)\ncard.push_to_hub(HF_REPO, commit_message=\"Add model card\")\nprint(f\"Model card pushed to https://huggingface.co/{HF_REPO}\")\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Clone and build llama.cpp for GGUF conversion\nimport os\nimport subprocess\nimport sys\n\nllama_dir = Path(\"/content/llama.cpp\")\nif llama_dir.exists():\n    print(\"llama.cpp already exists; refreshing checkout...\")\n    subprocess.check_call([\"git\", \"-C\", str(llama_dir), \"pull\", \"--ff-only\"])\nelse:\n    subprocess.check_call([\"git\", \"clone\", \"https://github.com/ggml-org/llama.cpp.git\", str(llama_dir)])\n\nsubprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-r\", str(llama_dir / \"requirements.txt\")])\n\n# Build llama-quantize\nsubprocess.check_call([\"cmake\", \"-B\", \"build\"], cwd=llama_dir)\nsubprocess.check_call([\"cmake\", \"--build\", \"build\", f\"-j{os.cpu_count() or 2}\"], cwd=llama_dir)\n\nquantize_bin = llama_dir / \"build/bin/llama-quantize\"\nassert quantize_bin.exists(), f\"llama-quantize not found at {quantize_bin}\"\nprint(f\"llama-quantize binary: {quantize_bin}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Convert to GGUF and quantize\nimport subprocess\nimport sys\n\ngguf_dir = OUTPUT_DIR / \"gguf\"\ngguf_dir.mkdir(parents=True, exist_ok=True)\n\nf16_path = gguf_dir / \"wx-afd-qwen3-4b-F16.gguf\"\nq8_path = gguf_dir / \"wx-afd-qwen3-4b-Q8_0.gguf\"\nq4_path = gguf_dir / \"wx-afd-qwen3-4b-Q4_K_M.gguf\"\n\n# Step 1: HF → F16 GGUF\nprint(\"Converting HF model to F16 GGUF ...\")\nsubprocess.check_call([\n    sys.executable,\n    \"/content/llama.cpp/convert_hf_to_gguf.py\",\n    str(merged_dir),\n    \"--outtype\",\n    \"f16\",\n    \"--outfile\",\n    str(f16_path),\n])\nassert f16_path.exists(), f\"F16 conversion failed: {f16_path}\"\nprint(f\"  F16: {f16_path.stat().st_size / 1e9:.2f} GB\")\n\n# Step 2: F16 → Q8_0\nprint(\"Quantizing F16 → Q8_0 ...\")\nsubprocess.check_call([str(quantize_bin), str(f16_path), str(q8_path), \"Q8_0\"])\nassert q8_path.exists(), f\"Q8_0 quantization failed: {q8_path}\"\nprint(f\"  Q8_0: {q8_path.stat().st_size / 1e9:.2f} GB\")\n\n# Step 3: F16 → Q4_K_M\nprint(\"Quantizing F16 → Q4_K_M ...\")\nsubprocess.check_call([str(quantize_bin), str(f16_path), str(q4_path), \"Q4_K_M\"])\nassert q4_path.exists(), f\"Q4_K_M quantization failed: {q4_path}\"\nprint(f\"  Q4_K_M: {q4_path.stat().st_size / 1e9:.2f} GB\")\n\nprint()\nprint(\"GGUF files (saved to Google Drive):\")\nfor p in [f16_path, q8_path, q4_path]:\n    print(f\"  {p.name}: {p.stat().st_size / 1e9:.2f} GB\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push GGUF files to HuggingFace Hub\n",
    "api.create_repo(GGUF_REPO, exist_ok=True)\n",
    "\n",
    "gguf_files = [f16_path, q8_path, q4_path]\n",
    "for gf in gguf_files:\n",
    "    print(f\"Uploading {gf.name} ({gf.stat().st_size / 1e9:.2f} GB) ...\")\n",
    "    api.upload_file(\n",
    "        path_or_fileobj=str(gf),\n",
    "        path_in_repo=gf.name,\n",
    "        repo_id=GGUF_REPO,\n",
    "        commit_message=f\"Upload {gf.name}\",\n",
    "    )\n",
    "    print(f\"  Uploaded: {gf.name}\")\n",
    "\n",
    "# Push GGUF model card\n",
    "gguf_card_text = f\"\"\"---\n",
    "license: apache-2.0\n",
    "base_model: Qwen/Qwen3-4B-Instruct-2507\n",
    "tags:\n",
    "  - weather\n",
    "  - meteorology\n",
    "  - nws\n",
    "  - gguf\n",
    "  - llama-cpp\n",
    "---\n",
    "\n",
    "# WX-AFD: Qwen3-4B GGUF Quantizations\n",
    "\n",
    "GGUF quantizations of [{HF_REPO}](https://huggingface.co/{HF_REPO}) for\n",
    "llama.cpp, Ollama, and LM Studio.\n",
    "\n",
    "## Available Files\n",
    "\n",
    "| File | Quant | Size |\n",
    "|------|-------|------|\n",
    "| `wx-afd-qwen3-4b-F16.gguf` | F16 | {f16_path.stat().st_size / 1e9:.2f} GB |\n",
    "| `wx-afd-qwen3-4b-Q8_0.gguf` | Q8_0 | {q8_path.stat().st_size / 1e9:.2f} GB |\n",
    "| `wx-afd-qwen3-4b-Q4_K_M.gguf` | Q4_K_M | {q4_path.stat().st_size / 1e9:.2f} GB |\n",
    "\n",
    "## Usage with llama.cpp\n",
    "\n",
    "```bash\n",
    "# Download\n",
    "huggingface-cli download {GGUF_REPO} wx-afd-qwen3-4b-Q4_K_M.gguf --local-dir .\n",
    "\n",
    "# Run\n",
    "llama-cli -m wx-afd-qwen3-4b-Q4_K_M.gguf -p \"<weather data>\" -n 2048\n",
    "```\n",
    "\n",
    "## Usage with Ollama\n",
    "\n",
    "```bash\n",
    "# Create Modelfile\n",
    "echo 'FROM ./wx-afd-qwen3-4b-Q4_K_M.gguf' > Modelfile\n",
    "ollama create wx-afd -f Modelfile\n",
    "ollama run wx-afd\n",
    "```\n",
    "\n",
    "See the [full model card](https://huggingface.co/{HF_REPO}) for evaluation results and training details.\n",
    "\"\"\"\n",
    "\n",
    "gguf_card = ModelCard(gguf_card_text)\n",
    "gguf_card.push_to_hub(GGUF_REPO, commit_message=\"Add GGUF model card\")\n",
    "print(f\"\\nGGUF repo: https://huggingface.co/{GGUF_REPO}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Next Steps\n",
    "\n",
    "**Completed in this notebook:**\n",
    "- Environment setup (Colab dependencies + Google Drive)\n",
    "- Data pipeline (clone repo + run scripts 01-03 if needed)\n",
    "- Data inspection and validation\n",
    "- Config verification with all corrections applied\n",
    "- Sanity check (loss masking, token IDs, dataset loading)\n",
    "- Model training (DoRA + rsLoRA, 3 epochs with early stopping)\n",
    "- LoRA merge and export\n",
    "- Full evaluation: ROUGE, BERTScore, format compliance\n",
    "- Zero-shot baseline comparison\n",
    "- Post-training sanity checks (6 gates)\n",
    "- HuggingFace Hub push: [ringusTheImp/wx-afd-qwen3-4b](https://huggingface.co/ringusTheImp/wx-afd-qwen3-4b)\n",
    "- GGUF quantizations (F16, Q8_0, Q4_K_M): [ringusTheImp/wx-afd-qwen3-4b-GGUF](https://huggingface.co/ringusTheImp/wx-afd-qwen3-4b-GGUF)\n",
    "\n",
    "**MLX conversion (run locally on Apple Silicon):**\n",
    "```bash\n",
    "pip install mlx-lm\n",
    "mlx_lm.convert --hf-path ringusTheImp/wx-afd-qwen3-4b -q --upload-repo ringusTheImp/wx-afd-qwen3-4b-MLX\n",
    "```\n",
    "\n",
    "**Future work:**\n",
    "- Multi-WFO training (expand beyond Louisville)\n",
    "- AlignScore factual consistency evaluation\n",
    "- GRPO/DPO alignment with AlignScore as reward signal\n",
    "- Attribute-specific LoRA adapters (synopsis vs aviation)\n",
    "- vLLM/TGI deployment for real-time inference"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
