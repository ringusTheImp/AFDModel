{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# WX-AFD: Fine-Tuning Qwen3-4B for Area Forecast Discussions\n\nThis notebook fine-tunes **Qwen3-4B-Instruct-2507** with DoRA + rsLoRA to generate NWS Area Forecast Discussions from structured weather data.\n\n**Google Colab version** — requires A100 GPU + High RAM runtime.\n\n**Pipeline context:** This is step 4 in the WX-AFD pipeline:\n1. `01_scrape_afds.py` — Scrape AFDs from IEM\n2. `02_fetch_weather.py` — Fetch weather data from Open-Meteo\n3. `03_build_dataset.py` — Build training JSONL (messages format)\n4. **`04_train_colab.ipynb` — Fine-tune, evaluate, and export model** (this notebook)\n\n---\n\n**Table of Contents**\n1. [Environment Setup](#1.-Environment-Setup)\n2. [Data Inspection](#2.-Data-Inspection)\n3. [Configuration](#3.-Configuration)\n4. [Sanity Check](#4.-Sanity-Check)\n5. [Training](#5.-Training)\n6. [Training Curves](#6.-Training-Curves)\n7. [LoRA Merge](#7.-LoRA-Merge)\n8. [Inference](#8.-Inference)\n9. [Evaluation](#9.-Evaluation)\n10. [Results](#10.-Results)\n11. [Post-Training Sanity Checks](#11.-Post-Training-Sanity-Checks)\n12. [Push to HuggingFace Hub](#12.-Push-to-HuggingFace-Hub)\n13. [Next Steps](#13.-Next-Steps)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install dependencies (torch is pre-installed on Colab — don't reinstall)\n!pip install -U packaging setuptools wheel ninja\n!pip install --no-build-isolation axolotl[flash-attn,deepspeed]\n!pip install rouge-score==0.1.2 bert-score sacrebleu"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive and set up paths\n",
    "from pathlib import Path\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "DRIVE_ROOT = Path(\"/content/drive/MyDrive/wx-afd\")\n",
    "DATA_DIR = DRIVE_ROOT / \"data\"\n",
    "CONFIG_PATH = DRIVE_ROOT / \"configs\" / \"wx-afd-dora.yml\"\n",
    "OUTPUT_DIR = DRIVE_ROOT / \"output\"\n",
    "EVAL_DIR = DRIVE_ROOT / \"eval\"\n",
    "\n",
    "# Ensure directories exist\n",
    "for d in [DATA_DIR, CONFIG_PATH.parent, OUTPUT_DIR, EVAL_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Drive root: {DRIVE_ROOT}\")\n",
    "print(f\"Data dir:   {DATA_DIR}\")\n",
    "print(f\"Config:     {CONFIG_PATH}\")\n",
    "print(f\"Output:     {OUTPUT_DIR}\")\n",
    "print(f\"Eval:       {EVAL_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import sys\n",
    "\n",
    "# Clone repo (for wx_afd.py, configs, and pipeline scripts)\n",
    "!git clone https://github.com/ringusTheImp/AFDModel.git /content/AFDModel\n",
    "sys.path.insert(0, \"/content/AFDModel\")\n",
    "\n",
    "# Check for existing data on Drive, run pipeline if missing\n",
    "if not (DATA_DIR / \"train.jsonl\").exists():\n",
    "    print(\"Data not found on Drive — running pipeline...\")\n",
    "    !cd /content/AFDModel && python 01_scrape_afds.py\n",
    "    !cd /content/AFDModel && python 02_fetch_weather.py\n",
    "    !cd /content/AFDModel && python 03_build_dataset.py\n",
    "    # Copy results to Drive for persistence\n",
    "    shutil.copytree(\"/content/AFDModel/data\", str(DATA_DIR), dirs_exist_ok=True)\n",
    "    print(f\"Data copied to Drive: {DATA_DIR}\")\n",
    "else:\n",
    "    print(f\"Data found on Drive: {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "# Write Colab-specific Axolotl config with Drive paths\n",
    "# Load the template from the cloned repo\n",
    "with open(\"/content/AFDModel/configs/wx-afd-dora.yml\") as f:\n",
    "    config_data = yaml.safe_load(f)\n",
    "\n",
    "# Patch paths for Colab + Google Drive\n",
    "config_data[\"datasets\"][0][\"path\"] = str(DATA_DIR / \"train.jsonl\")\n",
    "config_data[\"test_datasets\"][0][\"path\"] = str(DATA_DIR / \"val.jsonl\")\n",
    "config_data[\"output_dir\"] = str(OUTPUT_DIR)\n",
    "\n",
    "with open(CONFIG_PATH, \"w\") as f:\n",
    "    yaml.dump(config_data, f, default_flow_style=False, sort_keys=False)\n",
    "\n",
    "print(f\"Config written to: {CONFIG_PATH}\")\n",
    "print(f\"  datasets[0].path:      {config_data['datasets'][0]['path']}\")\n",
    "print(f\"  test_datasets[0].path: {config_data['test_datasets'][0]['path']}\")\n",
    "print(f\"  output_dir:            {config_data['output_dir']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "MODEL_ID = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
    "\n",
    "# ---- GPU check ----\n",
    "print(f\"PyTorch:  {torch.__version__}\")\n",
    "print(f\"CUDA:     {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU:      {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM:     {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "print(f\"Root:     {DRIVE_ROOT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Inspection\n",
    "\n",
    "Verify that our training data (output of `03_build_dataset.py`) is correctly formatted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonl(path):\n",
    "    \"\"\"Load JSONL file into a list of dicts.\"\"\"\n",
    "    with open(path) as f:\n",
    "        return [json.loads(line) for line in f]\n",
    "\n",
    "train_data = load_jsonl(DATA_DIR / \"train.jsonl\")\n",
    "val_data = load_jsonl(DATA_DIR / \"val.jsonl\")\n",
    "\n",
    "print(f\"Training examples:   {len(train_data)}\")\n",
    "print(f\"Validation examples: {len(val_data)}\")\n",
    "print(f\"Total:               {len(train_data) + len(val_data)}\")\n",
    "\n",
    "# Verify messages format\n",
    "ex = train_data[0]\n",
    "assert \"messages\" in ex, \"Missing 'messages' key\"\n",
    "assert len(ex[\"messages\"]) == 3, f\"Expected 3 messages, got {len(ex['messages'])}\"\n",
    "assert ex[\"messages\"][0][\"role\"] == \"system\"\n",
    "assert ex[\"messages\"][1][\"role\"] == \"user\"\n",
    "assert ex[\"messages\"][2][\"role\"] == \"assistant\"\n",
    "print(\"\\nMessages format: OK (system + user + assistant)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token length distribution\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "\n",
    "def count_tokens(example):\n",
    "    text = tokenizer.apply_chat_template(example[\"messages\"], tokenize=False)\n",
    "    return len(tokenizer.encode(text))\n",
    "\n",
    "train_lengths = [count_tokens(ex) for ex in train_data]\n",
    "val_lengths = [count_tokens(ex) for ex in val_data]\n",
    "all_lengths = train_lengths + val_lengths\n",
    "\n",
    "print(f\"Token lengths (all {len(all_lengths)} examples):\")\n",
    "print(f\"  Min:    {min(all_lengths)}\")\n",
    "print(f\"  Mean:   {sum(all_lengths) // len(all_lengths)}\")\n",
    "print(f\"  Median: {sorted(all_lengths)[len(all_lengths)//2]}\")\n",
    "print(f\"  Max:    {max(all_lengths)}\")\n",
    "print(f\"  >2048:  {sum(1 for l in all_lengths if l > 2048)} \"\n",
    "      f\"({sum(1 for l in all_lengths if l > 2048)/len(all_lengths):.1%})\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "ax.hist(all_lengths, bins=50, edgecolor=\"black\", alpha=0.7)\n",
    "ax.axvline(2048, color=\"red\", linestyle=\"--\", label=\"sequence_len=2048\")\n",
    "ax.set_xlabel(\"Total tokens per example\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "ax.set_title(\"Token Length Distribution\")\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a sample training example\n",
    "sample = train_data[0]\n",
    "print(\"=\" * 70)\n",
    "print(\"SYSTEM PROMPT:\")\n",
    "print(\"=\" * 70)\n",
    "print(sample[\"messages\"][0][\"content\"][:300], \"...\")\n",
    "print()\n",
    "print(\"=\" * 70)\n",
    "print(\"WEATHER INPUT (first 500 chars):\")\n",
    "print(\"=\" * 70)\n",
    "print(sample[\"messages\"][1][\"content\"][:500], \"...\")\n",
    "print()\n",
    "print(\"=\" * 70)\n",
    "print(\"AFD OUTPUT (first 500 chars):\")\n",
    "print(\"=\" * 70)\n",
    "print(sample[\"messages\"][2][\"content\"][:500], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuration\n",
    "\n",
    "Load and validate the Axolotl YAML config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(CONFIG_PATH) as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Validate critical fields\n",
    "checks = {\n",
    "    \"base_model\": config.get(\"base_model\") == \"Qwen/Qwen3-4B-Instruct-2507\",\n",
    "    \"chat_template\": config.get(\"chat_template\") == \"qwen3\",\n",
    "    \"eos_token\": config.get(\"special_tokens\", {}).get(\"eos_token\") == \"<|im_end|>\",\n",
    "    \"pad_token\": config.get(\"special_tokens\", {}).get(\"pad_token\") == \"<|endoftext|>\",\n",
    "    \"adapter\": config.get(\"adapter\") == \"lora\",\n",
    "    \"lora_r\": config.get(\"lora_r\") == 16,\n",
    "    \"lora_alpha\": config.get(\"lora_alpha\") == 32,\n",
    "    \"peft_use_dora\": config.get(\"peft_use_dora\") is True,\n",
    "    \"peft_use_rslora\": config.get(\"peft_use_rslora\") is True,\n",
    "    \"sample_packing\": config.get(\"sample_packing\") is True,\n",
    "    \"roles_to_train\": config[\"datasets\"][0].get(\"roles_to_train\") == [\"assistant\"],\n",
    "    \"eot_tokens\": config.get(\"eot_tokens\") == [\"<|im_end|>\"],\n",
    "    \"test_datasets\": \"test_datasets\" in config,\n",
    "    \"sequence_len\": config.get(\"sequence_len\") == 2048,\n",
    "    \"bf16\": config.get(\"bf16\") is True,\n",
    "    \"early_stopping\": config.get(\"early_stopping_patience\") == 5,\n",
    "}\n",
    "\n",
    "all_ok = True\n",
    "for name, ok in checks.items():\n",
    "    status = \"OK\" if ok else \"FAIL\"\n",
    "    if not ok:\n",
    "        all_ok = False\n",
    "    print(f\"  [{status}] {name}\")\n",
    "\n",
    "assert all_ok, \"Config validation failed — fix issues above before training\"\n",
    "print(\"\\nConfig validation: ALL PASSED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sanity Check\n",
    "\n",
    "Use Axolotl's internals to validate config normalization, dataset loading, and loss masking\n",
    "**before** launching a real training job. This catches issues early without GPU time.\n",
    "\n",
    "Key checks:\n",
    "- Config normalizes without errors\n",
    "- Dataset loads and tokenizes correctly\n",
    "- Labels are `-100` for system/user tokens (loss masking)\n",
    "- EOS/PAD token IDs are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from axolotl.utils.config import normalize_config, validate_config\nfrom axolotl.common.datasets import load_datasets\nfrom axolotl.utils.dict import DictDefault\n\n# Workaround: Axolotl 0.14.0 bug — SFTDataset pydantic model not subscriptable\nfrom axolotl.utils.schemas.config import SFTDataset\nif not hasattr(SFTDataset, '__getitem__'):\n    SFTDataset.__getitem__ = lambda self, key: getattr(self, key)\n\n# Normalize and validate\ncfg = DictDefault(config)\nnormalize_config(cfg)\nvalidate_config(cfg)\nprint(\"Config normalization: OK\")\nprint(\"Config validation:    OK\")\n\n# Load tokenizer for checks\ntok = AutoTokenizer.from_pretrained(cfg.base_model, trust_remote_code=True)\neos_id = tok.convert_tokens_to_ids(\"<|im_end|>\")\npad_id = tok.convert_tokens_to_ids(\"<|endoftext|>\")\nprint(f\"EOS token ID: {eos_id} (expect 151645) — {'OK' if eos_id == 151645 else 'FAIL'}\")\nprint(f\"PAD token ID: {pad_id} (expect 151643) — {'OK' if pad_id == 151643 else 'FAIL'}\")\nassert eos_id != pad_id, \"EOS and PAD must differ!\"\n\n# Load datasets (this tests the full data pipeline)\nprint(\"\\nLoading datasets...\")\ndataset_meta = load_datasets(cfg=cfg)\ntrain_dataset = dataset_meta.train_dataset\neval_dataset = dataset_meta.eval_dataset\nprint(f\"  Train: {len(train_dataset)} packed sequences\")\nif eval_dataset:\n    print(f\"  Eval:  {len(eval_dataset)} packed sequences\")\n\n# Verify loss masking: labels should be -100 for system/user tokens\nsample = train_dataset[0]\nlabels = sample[\"labels\"]\nn_masked = sum(1 for l in labels if l == -100)\nn_total = len(labels)\nprint(f\"\\nLoss masking (sample 0):\")\nprint(f\"  Total tokens:  {n_total}\")\nprint(f\"  Masked (-100): {n_masked} ({n_masked/n_total:.1%})\")\nprint(f\"  Trained:       {n_total - n_masked} ({(n_total - n_masked)/n_total:.1%})\")\nassert n_masked > 0, \"No masked tokens — loss masking may be broken\"\nprint(\"\\nSanity check: ALL PASSED\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training\n",
    "\n",
    "Run training directly in this notebook session (requires A100 GPU runtime).\n",
    "\n",
    "**Expected timeline:** ~429 steps across 3 epochs, ~45-75 minutes on A100 40GB.\n",
    "Early stopping (patience=5) may terminate after epoch 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from axolotl.train import train\n",
    "\n",
    "train(cfg=cfg, dataset_meta=dataset_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor training logs\n",
    "log_dir = OUTPUT_DIR / \"runs\"\n",
    "logs = sorted(log_dir.glob(\"**/events.*\")) if log_dir.exists() else []\n",
    "if logs:\n",
    "    print(f\"TensorBoard logs found: {len(logs)} event files\")\n",
    "    print(f\"  Directory: {log_dir}\")\n",
    "else:\n",
    "    print(\"No TensorBoard logs found yet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find trainer_state.json\n",
    "state_files = sorted(OUTPUT_DIR.glob(\"**/trainer_state.json\"))\n",
    "if not state_files:\n",
    "    print(\"No trainer_state.json found — training may not have completed yet.\")\n",
    "else:\n",
    "    state_path = state_files[-1]\n",
    "    print(f\"Loading: {state_path}\")\n",
    "    state = json.loads(state_path.read_text())\n",
    "\n",
    "    # Extract metrics\n",
    "    train_loss, train_steps = [], []\n",
    "    eval_loss, eval_steps = [], []\n",
    "    lr_values, lr_steps = [], []\n",
    "\n",
    "    for entry in state[\"log_history\"]:\n",
    "        step = entry[\"step\"]\n",
    "        if \"loss\" in entry:\n",
    "            train_loss.append(entry[\"loss\"])\n",
    "            train_steps.append(step)\n",
    "        if \"eval_loss\" in entry:\n",
    "            eval_loss.append(entry[\"eval_loss\"])\n",
    "            eval_steps.append(step)\n",
    "        if \"learning_rate\" in entry:\n",
    "            lr_values.append(entry[\"learning_rate\"])\n",
    "            lr_steps.append(step)\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    # Loss curves\n",
    "    ax1.plot(train_steps, train_loss, label=\"Train Loss\", alpha=0.7)\n",
    "    if eval_loss:\n",
    "        ax1.plot(eval_steps, eval_loss, label=\"Val Loss\", marker=\"o\", markersize=4)\n",
    "    ax1.set_xlabel(\"Step\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.set_title(\"Training & Validation Loss\")\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    # LR schedule\n",
    "    if lr_values:\n",
    "        ax2.plot(lr_steps, lr_values, color=\"green\")\n",
    "        ax2.set_xlabel(\"Step\")\n",
    "        ax2.set_ylabel(\"Learning Rate\")\n",
    "        ax2.set_title(\"Learning Rate Schedule\")\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Summary\n",
    "    print(f\"\\nTotal steps:     {state['global_step']}\")\n",
    "    print(f\"Best model step: {state.get('best_model_checkpoint', 'N/A')}\")\n",
    "    if eval_loss:\n",
    "        print(f\"Best val loss:   {min(eval_loss):.4f} (step {eval_steps[eval_loss.index(min(eval_loss))]})\")\n",
    "    print(f\"Final train loss: {train_loss[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 6. Training Curves",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7. LoRA Merge\n\nMerge the LoRA adapter back into the base model for clean inference.\nThe merged model is a standard HuggingFace directory — no adapter loading needed."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge LoRA adapter into base model\n",
    "merge_cmd = [\n",
    "    \"accelerate\", \"launch\", \"-m\", \"axolotl.cli.merge_lora\",\n",
    "    str(CONFIG_PATH),\n",
    "    \"--lora_model_dir\", str(OUTPUT_DIR),\n",
    "]\n",
    "print(f\"Running: {' '.join(merge_cmd)}\")\n",
    "result = subprocess.run(merge_cmd, capture_output=True, text=True)\n",
    "print(result.stdout)\n",
    "if result.returncode != 0:\n",
    "    print(f\"STDERR:\\n{result.stderr}\")\n",
    "    raise RuntimeError(\"Merge failed\")\n",
    "\n",
    "merged_dir = OUTPUT_DIR / \"merged\"\n",
    "assert merged_dir.exists(), f\"Merged model not found at {merged_dir}\"\n",
    "print(f\"\\nMerged model saved to: {merged_dir}\")\n",
    "print(f\"Contents: {[f.name for f in sorted(merged_dir.iterdir())]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merged model is already on Google Drive — no copy needed\n",
    "print(f\"Merged model saved to Google Drive: {OUTPUT_DIR / 'merged'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Inference\n",
    "\n",
    "Load the merged model and generate AFDs. Compare against reference forecasts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wx_afd import generate_afd, load_model\n",
    "\n",
    "# Load merged model\n",
    "merged_dir = OUTPUT_DIR / \"merged\"\n",
    "print(f\"Loading merged model from: {merged_dir}\")\n",
    "model, tokenizer = load_model(str(merged_dir))\n",
    "print(\"Model loaded.\")\n",
    "\n",
    "# Generate from a validation example\n",
    "val_ex = val_data[0]\n",
    "weather_input = val_ex[\"messages\"][1][\"content\"]\n",
    "reference_afd = val_ex[\"messages\"][2][\"content\"]\n",
    "\n",
    "generated_afd = generate_afd(model, tokenizer, weather_input)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"GENERATED AFD:\")\n",
    "print(\"=\" * 70)\n",
    "print(generated_afd[:1000], \"...\" if len(generated_afd) > 1000 else \"\")\n",
    "print()\n",
    "print(\"=\" * 70)\n",
    "print(\"REFERENCE AFD:\")\n",
    "print(\"=\" * 70)\n",
    "print(reference_afd[:1000], \"...\" if len(reference_afd) > 1000 else \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EOS behavior check: verify model stops generating\n",
    "print(\"EOS Behavior Check — generating 5 examples...\")\n",
    "print()\n",
    "for i in range(min(5, len(val_data))):\n",
    "    inp = val_data[i][\"messages\"][1][\"content\"]\n",
    "    out = generate_afd(model, tokenizer, inp)\n",
    "    terminated = len(out) < 2048 * 4  # rough char-level check\n",
    "    print(f\"  Example {i}: {len(out)} chars — {'OK (terminated)' if terminated else 'WARNING: may not have stopped'}\")\n",
    "\n",
    "print(\"\\nAll examples should terminate cleanly with <|im_end|>.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Evaluation\n",
    "\n",
    "Full evaluation on all validation examples using ROUGE-1/2/L, BERTScore F1, and format compliance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "from tqdm.notebook import tqdm\n",
    "from wx_afd import REQUIRED_SECTIONS, compute_rouge, compute_bertscore, format_compliance\n",
    "\n",
    "# Generate all predictions\n",
    "predictions = []\n",
    "references = []\n",
    "\n",
    "print(f\"Generating AFDs for {len(val_data)} validation examples...\")\n",
    "for ex in tqdm(val_data):\n",
    "    pred = generate_afd(model, tokenizer, ex[\"messages\"][1][\"content\"])\n",
    "    predictions.append(pred)\n",
    "    references.append(ex[\"messages\"][2][\"content\"])\n",
    "\n",
    "# Save generated AFDs\n",
    "ft_eval_dir = EVAL_DIR / \"finetuned\"\n",
    "ft_gen_dir = ft_eval_dir / \"generated\"\n",
    "ft_scores_dir = ft_eval_dir / \"scores\"\n",
    "ft_gen_dir.mkdir(parents=True, exist_ok=True)\n",
    "ft_scores_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for i, pred in enumerate(predictions):\n",
    "    (ft_gen_dir / f\"example_{i:04d}.txt\").write_text(pred)\n",
    "\n",
    "# ROUGE\n",
    "rouge_avg = compute_rouge(predictions, references)\n",
    "print(f\"\\nROUGE-1: {rouge_avg['rouge1']:.4f}\")\n",
    "print(f\"ROUGE-2: {rouge_avg['rouge2']:.4f}\")\n",
    "print(f\"ROUGE-L: {rouge_avg['rougeL']:.4f}\")\n",
    "\n",
    "# Per-example ROUGE (for later analysis)\n",
    "scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
    "rouge_results = {\"rouge1\": [], \"rouge2\": [], \"rougeL\": []}\n",
    "for pred, ref in zip(predictions, references):\n",
    "    result = scorer.score(ref, pred)\n",
    "    for key in rouge_results:\n",
    "        rouge_results[key].append(result[key].fmeasure)\n",
    "\n",
    "# Free model memory before BERTScore\n",
    "del model\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# BERTScore\n",
    "print(\"\\nComputing BERTScore (this may take a minute)...\")\n",
    "bertscore_f1 = compute_bertscore(predictions, references)\n",
    "print(f\"BERTScore F1: {bertscore_f1:.4f}\")\n",
    "\n",
    "# Format compliance\n",
    "compliance = [format_compliance(pred)[\"compliance_score\"] for pred in predictions]\n",
    "avg_compliance = sum(compliance) / len(compliance)\n",
    "print(f\"Format compliance: {avg_compliance:.2%}\")\n",
    "\n",
    "# Save metrics\n",
    "ft_metrics = {\n",
    "    \"tag\": \"finetuned\",\n",
    "    \"num_examples\": len(predictions),\n",
    "    \"rouge1\": rouge_avg[\"rouge1\"],\n",
    "    \"rouge2\": rouge_avg[\"rouge2\"],\n",
    "    \"rougeL\": rouge_avg[\"rougeL\"],\n",
    "    \"bertscore_f1\": bertscore_f1,\n",
    "    \"format_compliance\": avg_compliance,\n",
    "}\n",
    "with open(ft_scores_dir / \"metrics.json\", \"w\") as f:\n",
    "    json.dump(ft_metrics, f, indent=2)\n",
    "print(f\"\\nMetrics saved to {ft_scores_dir / 'metrics.json'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero-shot baseline evaluation\n",
    "print(f\"Loading zero-shot baseline: {MODEL_ID}\")\n",
    "model_zs, tokenizer_zs = load_model(MODEL_ID)\n",
    "\n",
    "zs_predictions = []\n",
    "print(f\"Generating zero-shot AFDs for {len(val_data)} examples...\")\n",
    "for ex in tqdm(val_data):\n",
    "    pred = generate_afd(model_zs, tokenizer_zs, ex[\"messages\"][1][\"content\"])\n",
    "    zs_predictions.append(pred)\n",
    "\n",
    "# Save\n",
    "zs_eval_dir = EVAL_DIR / \"zero-shot\"\n",
    "zs_gen_dir = zs_eval_dir / \"generated\"\n",
    "zs_scores_dir = zs_eval_dir / \"scores\"\n",
    "zs_gen_dir.mkdir(parents=True, exist_ok=True)\n",
    "zs_scores_dir.mkdir(parents=True, exist_ok=True)\n",
    "for i, pred in enumerate(zs_predictions):\n",
    "    (zs_gen_dir / f\"example_{i:04d}.txt\").write_text(pred)\n",
    "\n",
    "# ROUGE\n",
    "zs_rouge_avg = compute_rouge(zs_predictions, references)\n",
    "\n",
    "# Free model before BERTScore\n",
    "del model_zs\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# BERTScore\n",
    "print(\"Computing BERTScore for zero-shot...\")\n",
    "zs_bertscore = compute_bertscore(zs_predictions, references)\n",
    "\n",
    "# Compliance\n",
    "zs_compliance = [format_compliance(pred)[\"compliance_score\"] for pred in zs_predictions]\n",
    "zs_avg_compliance = sum(zs_compliance) / len(zs_compliance)\n",
    "\n",
    "zs_metrics = {\n",
    "    \"tag\": \"zero-shot\",\n",
    "    \"num_examples\": len(zs_predictions),\n",
    "    \"rouge1\": zs_rouge_avg[\"rouge1\"],\n",
    "    \"rouge2\": zs_rouge_avg[\"rouge2\"],\n",
    "    \"rougeL\": zs_rouge_avg[\"rougeL\"],\n",
    "    \"bertscore_f1\": zs_bertscore,\n",
    "    \"format_compliance\": zs_avg_compliance,\n",
    "}\n",
    "with open(zs_scores_dir / \"metrics.json\", \"w\") as f:\n",
    "    json.dump(zs_metrics, f, indent=2)\n",
    "\n",
    "print(f\"\\nZero-shot ROUGE-1: {zs_rouge_avg['rouge1']:.4f}\")\n",
    "print(f\"Zero-shot ROUGE-L: {zs_rouge_avg['rougeL']:.4f}\")\n",
    "print(f\"Zero-shot BERTScore F1: {zs_bertscore:.4f}\")\n",
    "print(f\"Zero-shot compliance: {zs_avg_compliance:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AFD format compliance detail\n",
    "print(\"AFD Section Presence (fine-tuned model):\")\n",
    "print()\n",
    "for sec in REQUIRED_SECTIONS:\n",
    "    present = sum(1 for p in predictions if sec in p.lower())\n",
    "    print(f\"  {sec:<15} {present}/{len(predictions)} ({present/len(predictions):.0%})\")\n",
    "\n",
    "print(\"\\nAFD Section Presence (zero-shot baseline):\")\n",
    "print()\n",
    "for sec in REQUIRED_SECTIONS:\n",
    "    present = sum(1 for p in zs_predictions if sec in p.lower())\n",
    "    print(f\"  {sec:<15} {present}/{len(zs_predictions)} ({present/len(zs_predictions):.0%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Results\n",
    "\n",
    "Compare fine-tuned model against zero-shot baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison table\n",
    "print(f\"{'Metric':<20} {'Fine-tuned':>12} {'Zero-shot':>12} {'Delta':>12}\")\n",
    "print(\"-\" * 58)\n",
    "for k in [\"rouge1\", \"rouge2\", \"rougeL\", \"bertscore_f1\", \"format_compliance\"]:\n",
    "    f_val = ft_metrics[k]\n",
    "    z_val = zs_metrics[k]\n",
    "    delta = f_val - z_val\n",
    "    sign = \"+\" if delta > 0 else \"\"\n",
    "    print(f\"{k:<20} {f_val:>12.4f} {z_val:>12.4f} {sign}{delta:>11.4f}\")\n",
    "\n",
    "# Bar chart\n",
    "import numpy as np\n",
    "\n",
    "metrics_keys = [\"rouge1\", \"rouge2\", \"rougeL\", \"bertscore_f1\", \"format_compliance\"]\n",
    "labels = [\"ROUGE-1\", \"ROUGE-2\", \"ROUGE-L\", \"BERTScore\\nF1\", \"Format\\nCompliance\"]\n",
    "ft_vals = [ft_metrics[k] for k in metrics_keys]\n",
    "zs_vals = [zs_metrics[k] for k in metrics_keys]\n",
    "\n",
    "x = np.arange(len(labels))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "bars1 = ax.bar(x - width/2, ft_vals, width, label=\"Fine-tuned\", color=\"#2196F3\")\n",
    "bars2 = ax.bar(x + width/2, zs_vals, width, label=\"Zero-shot\", color=\"#FF9800\")\n",
    "\n",
    "ax.set_ylabel(\"Score\")\n",
    "ax.set_title(\"Fine-tuned vs Zero-shot Evaluation\")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 1)\n",
    "ax.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best and worst 3 examples by ROUGE-L\n",
    "rougeL_scores = rouge_results[\"rougeL\"]\n",
    "indexed = sorted(enumerate(rougeL_scores), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TOP 3 (Best ROUGE-L):\")\n",
    "print(\"=\" * 70)\n",
    "for rank, (idx, score) in enumerate(indexed[:3], 1):\n",
    "    print(f\"\\n--- #{rank}: Example {idx} (ROUGE-L = {score:.4f}) ---\")\n",
    "    print(\"GENERATED (first 300 chars):\")\n",
    "    print(predictions[idx][:300])\n",
    "    print()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"BOTTOM 3 (Worst ROUGE-L):\")\n",
    "print(\"=\" * 70)\n",
    "for rank, (idx, score) in enumerate(indexed[-3:], 1):\n",
    "    print(f\"\\n--- #{rank}: Example {idx} (ROUGE-L = {score:.4f}) ---\")\n",
    "    print(\"GENERATED (first 300 chars):\")\n",
    "    print(predictions[idx][:300])\n",
    "    print(\"REFERENCE (first 300 chars):\")\n",
    "    print(references[idx][:300])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 11. Post-Training Sanity Checks\n\nSix gates that must **all pass** before publishing the model. These verify that the\nmerged model is structurally complete, generates properly terminated output, and\nmeets minimum quality thresholds relative to both absolute baselines and the\nzero-shot model.\n\n| Gate | Check |\n|------|-------|\n| 1 | Merged model directory contains required files |\n| 2 | Generated outputs terminate before `max_new_tokens` |\n| 3 | ROUGE-L > 0.15 |\n| 4 | BERTScore F1 > 0.50 |\n| 5 | Format compliance > 50% |\n| 6 | Fine-tuned metrics beat zero-shot baseline |",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gate 1: Merged model directory check\n",
    "print(\"Gate 1: Merged model directory ...\")\n",
    "merged_dir = OUTPUT_DIR / \"merged\"\n",
    "assert merged_dir.exists(), f\"Merged dir not found: {merged_dir}\"\n",
    "\n",
    "required_files = [\"config.json\", \"tokenizer_config.json\", \"tokenizer.json\"]\n",
    "for fname in required_files:\n",
    "    assert (merged_dir / fname).exists(), f\"Missing {fname} in merged dir\"\n",
    "\n",
    "safetensors = list(merged_dir.glob(\"*.safetensors\"))\n",
    "assert len(safetensors) >= 1, \"No .safetensors files in merged dir\"\n",
    "\n",
    "print(f\"  Directory: {merged_dir}\")\n",
    "print(f\"  Required files: {required_files} — all present\")\n",
    "print(f\"  Safetensors shards: {len(safetensors)}\")\n",
    "print(\"Gate 1: PASSED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gate 2: EOS termination check\n",
    "# Model was del'd during eval for BERTScore memory — reload it\n",
    "print(\"Gate 2: EOS termination ...\")\n",
    "model, tokenizer = load_model(str(merged_dir))\n",
    "\n",
    "MAX_NEW_TOKENS = 2048\n",
    "MARGIN = 10\n",
    "test_examples = val_data[:3]\n",
    "\n",
    "for i, ex in enumerate(test_examples):\n",
    "    weather_input = ex[\"messages\"][1][\"content\"]\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert NWS meteorologist.\"},\n",
    "        {\"role\": \"user\", \"content\": weather_input},\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=MAX_NEW_TOKENS,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            eos_token_id=tokenizer.convert_tokens_to_ids(\"<|im_end|>\"),\n",
    "            pad_token_id=tokenizer.convert_tokens_to_ids(\"<|endoftext|>\"),\n",
    "        )\n",
    "    generated_tokens = output[0][inputs[\"input_ids\"].shape[1]:]\n",
    "    n_gen = len(generated_tokens)\n",
    "    assert n_gen < MAX_NEW_TOKENS - MARGIN, (\n",
    "        f\"Example {i}: generated {n_gen} tokens (limit {MAX_NEW_TOKENS}), \"\n",
    "        f\"model may not be terminating with EOS\"\n",
    "    )\n",
    "    print(f\"  Example {i}: {n_gen} tokens — OK\")\n",
    "\n",
    "# Free model again\n",
    "del model\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Gate 2: PASSED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gates 3-6: Metric thresholds\n",
    "print(\"Gate 3: ROUGE-L > 0.15 ...\")\n",
    "assert ft_metrics[\"rougeL\"] > 0.15, (\n",
    "    f\"ROUGE-L = {ft_metrics['rougeL']:.4f} (threshold: 0.15)\"\n",
    ")\n",
    "print(f\"  ROUGE-L = {ft_metrics['rougeL']:.4f} — PASSED\")\n",
    "\n",
    "print(\"Gate 4: BERTScore F1 > 0.50 ...\")\n",
    "assert ft_metrics[\"bertscore_f1\"] > 0.50, (\n",
    "    f\"BERTScore F1 = {ft_metrics['bertscore_f1']:.4f} (threshold: 0.50)\"\n",
    ")\n",
    "print(f\"  BERTScore F1 = {ft_metrics['bertscore_f1']:.4f} — PASSED\")\n",
    "\n",
    "print(\"Gate 5: Format compliance > 0.50 ...\")\n",
    "assert ft_metrics[\"format_compliance\"] > 0.50, (\n",
    "    f\"Format compliance = {ft_metrics['format_compliance']:.4f} (threshold: 0.50)\"\n",
    ")\n",
    "print(f\"  Format compliance = {ft_metrics['format_compliance']:.4f} — PASSED\")\n",
    "\n",
    "print(\"Gate 6: Fine-tuned > zero-shot ...\")\n",
    "assert ft_metrics[\"rougeL\"] > zs_metrics[\"rougeL\"], (\n",
    "    f\"ROUGE-L: fine-tuned ({ft_metrics['rougeL']:.4f}) <= \"\n",
    "    f\"zero-shot ({zs_metrics['rougeL']:.4f})\"\n",
    ")\n",
    "assert ft_metrics[\"bertscore_f1\"] > zs_metrics[\"bertscore_f1\"], (\n",
    "    f\"BERTScore F1: fine-tuned ({ft_metrics['bertscore_f1']:.4f}) <= \"\n",
    "    f\"zero-shot ({zs_metrics['bertscore_f1']:.4f})\"\n",
    ")\n",
    "print(f\"  ROUGE-L:     {ft_metrics['rougeL']:.4f} > {zs_metrics['rougeL']:.4f} — PASSED\")\n",
    "print(f\"  BERTScore:   {ft_metrics['bertscore_f1']:.4f} > {zs_metrics['bertscore_f1']:.4f} — PASSED\")\n",
    "\n",
    "print()\n",
    "print(\"=\" * 60)\n",
    "print(\"ALL SANITY CHECKS PASSED — safe to push to HF Hub\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Push to HuggingFace Hub\n",
    "\n",
    "Upload the merged model and GGUF quantizations to HuggingFace:\n",
    "\n",
    "| Repo | Contents |\n",
    "|------|----------|\n",
    "| `ringusTheImp/wx-afd-qwen3-4b` | Merged HF model + tokenizer |\n",
    "| `ringusTheImp/wx-afd-qwen3-4b-GGUF` | F16, Q8_0, Q4_K_M quantizations |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install huggingface_hub and log in\n",
    "!pip install -U huggingface_hub python-dotenv\n",
    "\n",
    "from huggingface_hub import HfApi, login\n",
    "\n",
    "hf_token = None\n",
    "\n",
    "# 1. Try Colab Secrets\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    hf_token = userdata.get(\"HF_TOKEN\")\n",
    "    print(\"Found HF_TOKEN in Colab Secrets.\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# 2. Try .env file (for local runs)\n",
    "if hf_token is None:\n",
    "    try:\n",
    "        from dotenv import load_dotenv\n",
    "        load_dotenv()\n",
    "        hf_token = os.environ.get(\"HF_TOKEN\")\n",
    "        if hf_token:\n",
    "            print(\"Found HF_TOKEN in .env file.\")\n",
    "    except ImportError:\n",
    "        pass\n",
    "\n",
    "# 3. Fall back to interactive login\n",
    "if hf_token:\n",
    "    login(token=hf_token)\n",
    "else:\n",
    "    print(\"HF_TOKEN not found — using interactive login.\")\n",
    "    login()\n",
    "\n",
    "api = HfApi()\n",
    "whoami = api.whoami()\n",
    "print(f\"Logged in as: {whoami['name']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push merged model to HuggingFace Hub\n",
    "from huggingface_hub import ModelCard\n",
    "\n",
    "HF_REPO = \"ringusTheImp/wx-afd-qwen3-4b\"\n",
    "GGUF_REPO = \"ringusTheImp/wx-afd-qwen3-4b-GGUF\"\n",
    "\n",
    "# Build model card\n",
    "model_card_text = f\"\"\"---\n",
    "license: apache-2.0\n",
    "base_model: Qwen/Qwen3-4B-Instruct-2507\n",
    "tags:\n",
    "  - weather\n",
    "  - meteorology\n",
    "  - nws\n",
    "  - area-forecast-discussion\n",
    "  - dora\n",
    "  - rslora\n",
    "  - axolotl\n",
    "datasets:\n",
    "  - custom\n",
    "language:\n",
    "  - en\n",
    "---\n",
    "\n",
    "# WX-AFD: Qwen3-4B for NWS Area Forecast Discussions\n",
    "\n",
    "Fine-tuned **Qwen3-4B-Instruct-2507** with DoRA + rsLoRA to generate\n",
    "NWS Area Forecast Discussions (AFDs) from structured weather model data.\n",
    "Trained on Louisville, KY (WFO LMK) forecasts.\n",
    "\n",
    "## Evaluation Results\n",
    "\n",
    "| Metric | Fine-tuned | Zero-shot | Delta |\n",
    "|--------|-----------|-----------|-------|\n",
    "| ROUGE-1 | {ft_metrics['rouge1']:.4f} | {zs_metrics['rouge1']:.4f} | {ft_metrics['rouge1'] - zs_metrics['rouge1']:+.4f} |\n",
    "| ROUGE-2 | {ft_metrics['rouge2']:.4f} | {zs_metrics['rouge2']:.4f} | {ft_metrics['rouge2'] - zs_metrics['rouge2']:+.4f} |\n",
    "| ROUGE-L | {ft_metrics['rougeL']:.4f} | {zs_metrics['rougeL']:.4f} | {ft_metrics['rougeL'] - zs_metrics['rougeL']:+.4f} |\n",
    "| BERTScore F1 | {ft_metrics['bertscore_f1']:.4f} | {zs_metrics['bertscore_f1']:.4f} | {ft_metrics['bertscore_f1'] - zs_metrics['bertscore_f1']:+.4f} |\n",
    "| Format Compliance | {ft_metrics['format_compliance']:.2%} | {zs_metrics['format_compliance']:.2%} | {ft_metrics['format_compliance'] - zs_metrics['format_compliance']:+.2%} |\n",
    "\n",
    "## Usage\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"{HF_REPO}\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"{HF_REPO}\")\n",
    "\n",
    "messages = [\n",
    "    {{\"role\": \"system\", \"content\": \"You are an expert NWS meteorologist at the Louisville, Kentucky Weather Forecast Office (WFO LMK).\"}},\n",
    "    {{\"role\": \"user\", \"content\": \"<your weather data here>\"}},\n",
    "]\n",
    "text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "output = model.generate(**inputs, max_new_tokens=2048, temperature=0.7)\n",
    "print(tokenizer.decode(output[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True))\n",
    "```\n",
    "\n",
    "## GGUF Quantizations\n",
    "\n",
    "GGUF files for llama.cpp / Ollama / LM Studio are available at\n",
    "[{GGUF_REPO}](https://huggingface.co/{GGUF_REPO}):\n",
    "- `wx-afd-qwen3-4b-F16.gguf` — Full FP16\n",
    "- `wx-afd-qwen3-4b-Q8_0.gguf` — 8-bit quantization\n",
    "- `wx-afd-qwen3-4b-Q4_K_M.gguf` — 4-bit quantization (recommended for CPU)\n",
    "\n",
    "## MLX (Apple Silicon)\n",
    "\n",
    "MLX conversion requires macOS with Apple Silicon — run locally:\n",
    "\n",
    "```bash\n",
    "pip install mlx-lm\n",
    "mlx_lm.convert --hf-path {HF_REPO} -q --upload-repo ringusTheImp/wx-afd-qwen3-4b-MLX\n",
    "```\n",
    "\n",
    "## Training Details\n",
    "\n",
    "- **Base model:** Qwen/Qwen3-4B-Instruct-2507\n",
    "- **Method:** DoRA + rsLoRA (rank 16, alpha 32)\n",
    "- **Framework:** Axolotl\n",
    "- **Data:** {ft_metrics['num_examples']} validation examples from Louisville, KY WFO (LMK)\n",
    "- **Sequence length:** 2048 tokens (sample packing)\n",
    "- **Precision:** bfloat16\n",
    "\n",
    "## Citation\n",
    "\n",
    "```bibtex\n",
    "@misc{{wx-afd-qwen3-4b,\n",
    "  title={{WX-AFD: Fine-Tuning Qwen3-4B for Area Forecast Discussions}},\n",
    "  author={{ringusTheImp}},\n",
    "  year={{2025}},\n",
    "  url={{https://huggingface.co/{HF_REPO}}}\n",
    "}}\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "# Create repo and upload model\n",
    "api.create_repo(HF_REPO, exist_ok=True)\n",
    "print(f\"Uploading merged model to {HF_REPO} ...\")\n",
    "api.upload_folder(\n",
    "    folder_path=str(merged_dir),\n",
    "    repo_id=HF_REPO,\n",
    "    commit_message=\"Upload merged WX-AFD model\",\n",
    ")\n",
    "print(\"Model uploaded.\")\n",
    "\n",
    "# Push model card\n",
    "card = ModelCard(model_card_text)\n",
    "card.push_to_hub(HF_REPO, commit_message=\"Add model card\")\n",
    "print(f\"Model card pushed to https://huggingface.co/{HF_REPO}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone and build llama.cpp for GGUF conversion\n",
    "!git clone https://github.com/ggml-org/llama.cpp.git /content/llama.cpp\n",
    "!pip install -r /content/llama.cpp/requirements.txt\n",
    "\n",
    "# Build llama-quantize\n",
    "!cd /content/llama.cpp && cmake -B build && cmake --build build -j$(nproc)\n",
    "\n",
    "quantize_bin = Path(\"/content/llama.cpp/build/bin/llama-quantize\")\n",
    "assert quantize_bin.exists(), f\"llama-quantize not found at {quantize_bin}\"\n",
    "print(f\"llama-quantize binary: {quantize_bin}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to GGUF and quantize\n",
    "gguf_dir = OUTPUT_DIR / \"gguf\"\n",
    "gguf_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "f16_path = gguf_dir / \"wx-afd-qwen3-4b-F16.gguf\"\n",
    "q8_path = gguf_dir / \"wx-afd-qwen3-4b-Q8_0.gguf\"\n",
    "q4_path = gguf_dir / \"wx-afd-qwen3-4b-Q4_K_M.gguf\"\n",
    "\n",
    "# Step 1: HF → F16 GGUF\n",
    "print(\"Converting HF model to F16 GGUF ...\")\n",
    "!python /content/llama.cpp/convert_hf_to_gguf.py \\\n",
    "    {str(merged_dir)} \\\n",
    "    --outtype f16 \\\n",
    "    --outfile {str(f16_path)}\n",
    "assert f16_path.exists(), f\"F16 conversion failed: {f16_path}\"\n",
    "print(f\"  F16: {f16_path.stat().st_size / 1e9:.2f} GB\")\n",
    "\n",
    "# Step 2: F16 → Q8_0\n",
    "print(\"Quantizing F16 → Q8_0 ...\")\n",
    "!{quantize_bin} {str(f16_path)} {str(q8_path)} Q8_0\n",
    "assert q8_path.exists(), f\"Q8_0 quantization failed: {q8_path}\"\n",
    "print(f\"  Q8_0: {q8_path.stat().st_size / 1e9:.2f} GB\")\n",
    "\n",
    "# Step 3: F16 → Q4_K_M\n",
    "print(\"Quantizing F16 → Q4_K_M ...\")\n",
    "!{quantize_bin} {str(f16_path)} {str(q4_path)} Q4_K_M\n",
    "assert q4_path.exists(), f\"Q4_K_M quantization failed: {q4_path}\"\n",
    "print(f\"  Q4_K_M: {q4_path.stat().st_size / 1e9:.2f} GB\")\n",
    "\n",
    "print()\n",
    "print(\"GGUF files (saved to Google Drive):\")\n",
    "for p in [f16_path, q8_path, q4_path]:\n",
    "    print(f\"  {p.name}: {p.stat().st_size / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push GGUF files to HuggingFace Hub\n",
    "api.create_repo(GGUF_REPO, exist_ok=True)\n",
    "\n",
    "gguf_files = [f16_path, q8_path, q4_path]\n",
    "for gf in gguf_files:\n",
    "    print(f\"Uploading {gf.name} ({gf.stat().st_size / 1e9:.2f} GB) ...\")\n",
    "    api.upload_file(\n",
    "        path_or_fileobj=str(gf),\n",
    "        path_in_repo=gf.name,\n",
    "        repo_id=GGUF_REPO,\n",
    "        commit_message=f\"Upload {gf.name}\",\n",
    "    )\n",
    "    print(f\"  Uploaded: {gf.name}\")\n",
    "\n",
    "# Push GGUF model card\n",
    "gguf_card_text = f\"\"\"---\n",
    "license: apache-2.0\n",
    "base_model: Qwen/Qwen3-4B-Instruct-2507\n",
    "tags:\n",
    "  - weather\n",
    "  - meteorology\n",
    "  - nws\n",
    "  - gguf\n",
    "  - llama-cpp\n",
    "---\n",
    "\n",
    "# WX-AFD: Qwen3-4B GGUF Quantizations\n",
    "\n",
    "GGUF quantizations of [{HF_REPO}](https://huggingface.co/{HF_REPO}) for\n",
    "llama.cpp, Ollama, and LM Studio.\n",
    "\n",
    "## Available Files\n",
    "\n",
    "| File | Quant | Size |\n",
    "|------|-------|------|\n",
    "| `wx-afd-qwen3-4b-F16.gguf` | F16 | {f16_path.stat().st_size / 1e9:.2f} GB |\n",
    "| `wx-afd-qwen3-4b-Q8_0.gguf` | Q8_0 | {q8_path.stat().st_size / 1e9:.2f} GB |\n",
    "| `wx-afd-qwen3-4b-Q4_K_M.gguf` | Q4_K_M | {q4_path.stat().st_size / 1e9:.2f} GB |\n",
    "\n",
    "## Usage with llama.cpp\n",
    "\n",
    "```bash\n",
    "# Download\n",
    "huggingface-cli download {GGUF_REPO} wx-afd-qwen3-4b-Q4_K_M.gguf --local-dir .\n",
    "\n",
    "# Run\n",
    "llama-cli -m wx-afd-qwen3-4b-Q4_K_M.gguf -p \"<weather data>\" -n 2048\n",
    "```\n",
    "\n",
    "## Usage with Ollama\n",
    "\n",
    "```bash\n",
    "# Create Modelfile\n",
    "echo 'FROM ./wx-afd-qwen3-4b-Q4_K_M.gguf' > Modelfile\n",
    "ollama create wx-afd -f Modelfile\n",
    "ollama run wx-afd\n",
    "```\n",
    "\n",
    "See the [full model card](https://huggingface.co/{HF_REPO}) for evaluation results and training details.\n",
    "\"\"\"\n",
    "\n",
    "gguf_card = ModelCard(gguf_card_text)\n",
    "gguf_card.push_to_hub(GGUF_REPO, commit_message=\"Add GGUF model card\")\n",
    "print(f\"\\nGGUF repo: https://huggingface.co/{GGUF_REPO}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Next Steps\n",
    "\n",
    "**Completed in this notebook:**\n",
    "- Environment setup (Colab dependencies + Google Drive)\n",
    "- Data pipeline (clone repo + run scripts 01-03 if needed)\n",
    "- Data inspection and validation\n",
    "- Config verification with all corrections applied\n",
    "- Sanity check (loss masking, token IDs, dataset loading)\n",
    "- Model training (DoRA + rsLoRA, 3 epochs with early stopping)\n",
    "- LoRA merge and export\n",
    "- Full evaluation: ROUGE, BERTScore, format compliance\n",
    "- Zero-shot baseline comparison\n",
    "- Post-training sanity checks (6 gates)\n",
    "- HuggingFace Hub push: [ringusTheImp/wx-afd-qwen3-4b](https://huggingface.co/ringusTheImp/wx-afd-qwen3-4b)\n",
    "- GGUF quantizations (F16, Q8_0, Q4_K_M): [ringusTheImp/wx-afd-qwen3-4b-GGUF](https://huggingface.co/ringusTheImp/wx-afd-qwen3-4b-GGUF)\n",
    "\n",
    "**MLX conversion (run locally on Apple Silicon):**\n",
    "```bash\n",
    "pip install mlx-lm\n",
    "mlx_lm.convert --hf-path ringusTheImp/wx-afd-qwen3-4b -q --upload-repo ringusTheImp/wx-afd-qwen3-4b-MLX\n",
    "```\n",
    "\n",
    "**Future work:**\n",
    "- Multi-WFO training (expand beyond Louisville)\n",
    "- AlignScore factual consistency evaluation\n",
    "- GRPO/DPO alignment with AlignScore as reward signal\n",
    "- Attribute-specific LoRA adapters (synopsis vs aviation)\n",
    "- vLLM/TGI deployment for real-time inference"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}